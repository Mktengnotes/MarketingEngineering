# Modelos Estructurales

## Introducción a Modelos Estructurales

### Introducción

En esencia, un modelo econométrico estructural es aquel que deriva relaciones estimables estadísticamente a partir de supuestos bien definidos de comportamiento de los agentes que deciden respecto a las cantidades observables. En contraposición a los modelos estructurales están los modelos de forma reducida donde los modelos simplemente describen la variabilidad de alguna medida de interés en base a un conjunto de variables observables exógenas.

La disciplina económica suele llamar modelos estructurales a los resultantes de asumir que los consumidores maximizan una utilidad subyacente y que las firmas maximizan su rentabilidad esperada. Desde el marketing, se considera también en la definición aquellos que postulan hipótesis alternativas de comportamiento incluyendo así una variedad de teorías de comportamiento que nutren la disciplina tales como teoría de prospectos, contabilidad mental, elección sobre conjuntos de consideración, etc. Como se discutirá más adelante, no existe un modelo estructural puro y la línea que los separa de los modelos de forma reducida es ciertamente difusa.

Se incluirá en la discusión de modelos estructurales a cualquiera que considere alguna historia de comportamiento que permita añadir interpretabilidad a los parámetros del modelo.

**Ejemplo 1:** Supóngase que un analista busca estudiar cómo el precio en la región $i$ $(p_i)$ se ve afectado por la presencia o no de competencia. Si además de los precios se observa la cantidad de clientes en la región $(POP_i)$, el ingreso per cápita en la región $(INC_i)$ y una indicatriz $CMP_i$ que toma el valor 1 si en la región correspondiente presenta competencia (0 en caso contrario).

Entonces, un modelo de forma reducida sencillo para estudiar el problema viene dado por:

$$p_i = \beta_0+ \beta_1POP_i + \beta_2INC_i + \beta_3CMP_i + \varepsilon_i$$
Bajo este enfoque, se pueden usar técnicas de regresión tradicionales para estimar $\beta_3$ que en principio indicaría el impacto de la competencia en el nivel de precios. Sin embargo, la presencia de competencia en un determinado mercado depende también del nivel de precios. Si los precios en una región son altos, la rentabilidad esperada por entrar también es alta motivando a potenciales competidores a participar. En consecuencia, un modelo como el planteado podría subestimar
el efecto de la competencia.

Un modelo estructural buscaría derivar relaciones estimables a partir de supuestos básicos del comportamiento de la firma. Por ejemplo, se podría asumir que cada firma decide conjuntamente la entrada/salida de un mercado y los precios a cobrar de modo de maximizar la rentabilidad esperada.

**Ejemplo 2:** Supóngase que se busca describir la productividad de los miembros de la fuerza de venta medida como número de unidades vendidas $q$.

$$q=f(X,\beta)+\varepsilon$$

La especificación del término de error $\varepsilon$ puede por sí solo permitir dar una interpretación estructural a los estimadores. Si simplemente se asume un error normalmente distribuido, entonces corresponderá simplemente a un ruido blanco y la regresión simplemente indicará a través de los parámetros $\beta$ cómo las variables $X$ en promedio afectan las ventas $q$. Por el contrario, si se asume que el término $\varepsilon$ considera además del ruido una componente no observable positiva asociada a la brecha de productividad de los miembros menos eficientes de la fuerza de venta, entonces la regresión describirá la frontera eficiente de ventas. Esto puede hacerse por ejemplo especificando que $\varepsilon = \epsilon - \xi$ donde $\epsilon$ está normalmente distribuida centrada en cero, pero $\xi$ proviene de una normal truncada en los números positivos (este enfoque se le suele llamar de regresión estocástica de frontera).

El gran desafío de la aplicación de modelos econométricos a problemas comerciales es enriquecer el conocimiento respecto a cómo se comportan los agentes relevantes del negocio, para así tomar decisiones más consistentes y más rentables. Desde este punto de vista, se apunta a modelos que describan la lógica que determina el comportamiento de los clientes y firmas más allá de simples correlaciones estadísticas entre las variables observables. En general, son varias las ventajas de usar modelos estructurales por sobre modelos de forma reducida:

1. *La capacidad de contar una mejor historia del comportamiento de los agentes*. Esto se expresa por la capacidad de interpretación directa de los parámetros del modelo. Mientras los parámetros asociados a enfoques de regresión tradicionales típicamente indican la magnitud en que en promedio varía alguna magnitud de interés ante variaciones de otra, los parámetros de un modelo estructural indican entre otros la valoración relativa de un atributo en la función de utilidad, los precios de referencia de un producto o la aversión al riesgo de un tomador de decisión. La provisión de una historia de comportamiento más completa no se deriva exclusivamente de la interpretación directa de los parámetros del modelo sino que también de la capacidad de derivar métricas complementarias tales como elasticidades y excedentes de consumidores. Más aún, se puede proyectar el comportamiento para calcular probabilidades y frecuencias de compra, participaciones de mercado, etc.

2. La generación de estimaciones consistentes con las expectativas de los analistas. Frecuentemente, al analizar los datos se quiere dejar la mayor libertad posible al modelo *para dejar que la data hable*. Este enfoque puede tener valor y ser recomendable en estudios exploratorios, pero para tomar decisiones se necesitan estimaciones robustas y usar tanta información como sea posible. Las teorías usadas para derivar modelos econométricos estructurales suelen estar soportadas tanto por estudios experimentales como por amplia evidencia empírica en múltiples dominios. Por lo tanto, al incorporar teoría se está implícitamente usando información que ha demostrado consistentemente su validez.

**Ejemplo 3:** Supóngase que se quiere proponer un modelo que describa la participación de mercado de las distintas marcas en una industria. Si se usa un enfoque de regresión en que simplemente se disponen los _shares_ al lado izquierdo y una forma funcional flexible al lado derecho, el modelo resultante podría predecir participaciones fuera del rango [0,1], que difícilmente pueden justificarse. Por el contrario si se adscribe al axioma de elección de Luce (1959) que indica que la probabilidad de elección en un determinado conjunto depende del ratio entre una medida de atracción de la alternativa con respecto al atractivo total del conjunto, se fuerza a que las participaciones siempre estén en el rango deseado.

**Ejemplo 4:** La teoría económica predice que en general, las cantidades demandadas decrecen ante aumentos en su precio. Sin embargo, en muchas situaciones prácticas la disponibilidad de datos al nivel de agregación requerido es limitada dificultando la estimación de esta relación inversa entre precio y demanda. En estas situaciones no es raro que un modelo flexible prediga que la demanda crece en función del precio. Agregar estructura permite limitar la búsqueda solo entre aquellos modelos que son consistentes con la premisa de que las demandas decrecen en el precio. 

3. *Evaluación de impacto de modificación de políticas*. Una de las herramientas fundamentales de la función comercial es la generación de planes comerciales que buscan proponer un diseño del conjunto producto, plaza, precio y promoción que genere el mayor valor para el cliente y la captura del mayor excedente por parte de la firma. El rol de los modelos econométricos es estudiar el impacto que tendrían distintas estrategias en el comportamiento del consumidor. En esencia, un plan de marketing propone un cambio en las reglas del juego que han generado la data que se observa y por tanto se necesita apuntar a estimar los elementos más básicos del comportamiento que se mantendrán invariantes ante modificación de productos, precios, canales de distribución, etc. En este grupo se tienen valoraciones por atributos de productos, costo de transporte, aversión al riesgo, entre otros, que no pueden ser estimados a menos que se derive el modelo a partir de teorías individuales de comportamiento. En otras palabras, la derivación de modelos de demanda a partir de teorías de comportamiento permite evaluar contrafactuales que apoyan el diseño de propuestas de valor efectivas.

La necesidad de evaluar contrafactuales usando elementos fundamentales que no se vean afectados por cambios en los sistemas fue inicialmente discutida por Robert Lucas (1976) en la famosa crítica que lleva su nombre. En el contexto de la predicción de efectos macroeconómicos, Lucas postuló que cualquier cambio en las políticas variaría sistemáticamente la estructura de los modelos y por tanto se debe apuntar a describir parámetros profundos que gobiernan el comportamiento individual.

**Ejemplo 5:** Considere un retailer que vende múltiples productos a través de dos canales, las salas de venta tradicionales y un sitio web con despacho directo. El retailer está evaluando la posibilidad de reasignar el conjunto de productos que vende a través de cada canal para aumentar la rentabilidad del negocio. Para apoyar esta decisión, parece evidente que el simple análisis de las ventas de cada producto en cada canal no ayudará a predecir cómo dichos productos se venderían en el otro canal o cómo se afectaría la venta si un producto deja de venderse en algunos de los canales. Para hacer este ejercicio necesariamente se deberá investigar primitivas más fundamentales del comportamiento como preferencias intrínsecas por canal para cada categoría y patrones de sustitución entre las alternativas disponibles dentro del canal y con respecto al otro canal.

**Ejemplo 6:** En muchas industrias como la de vestuario de moda o de artículos tecnológicos, hay una alta variabilidad de la oferta con constantes entradas y salidas de diferentes versiones de los productos dificultando la proyección del desempeño de cada variante en el tiempo. Mientras el surtido de producto varía con frecuencia, hay parámetros de la demanda que pueden perdurar por varias temporadas tales como la elasticidad al precio, crecimiento de la categoría, factores estacionales y de sustitución/complementariedad de atributos. Un enfoque estructural apunta precisamente a la estimación de estos parámetros estables.

4. *Testear aplicabilidad de teoría*. Al usar un enfoque estructural, se fuerza a pensar detalladamente respecto al problema y explicitar cada uno de los supuestos de comportamiento. Las especificaciones alternativas de modelos de forma reducida simplemente corresponden a formas funcionales diferentes y por tanto no son informativas respecto a la lógica en que deciden los agentes. Por otra parte, dos modelos estructurales diferentes provienen de supuestos de comportamiento diferentes y por tanto cuando uno de ellos ajusta mejor a la data indica que hay una teoría de comportamiento que es más plausible que la otra en el dominio de aplicación del modelo. Así, los modelos estructurales no solo se nutren de teoría sino que también ayudan a su desarrollo.

Las ventajas antes descritas no implican que siempre deban preferirse modelos estructurales por sobre los de forma reducida. Como se ha descrito, los modelos de forma reducida suelen proveer suficiente flexibilidad para dejar que sea la data la que hable, lo que puede ser particularmente útil en análisis exploratorios del caso bajo estudio. Además, muchas veces la inclusión de más estructura en el modelo implica rutinas de estimación más sofisticadas siendo con frecuencia altamente intensivas computacionalmente.

Es importante destacar que no existe un modelo puramente estructural. Todo modelo requiere en algún momento suponer alguna forma funcional flexible sin fundamento teórico sólido. Por ejemplo, se puede asumir que los consumidores al elegir un producto están maximizando una utilidad subyacente, pero ¿cómo describir dicha función de utilidad? ¿Qué variables explicativas usar y cuál forma funcional escoger? Ciertamente la especificidad de las teorías disponibles no alcanza a responder a estas preguntas y se debe por lo tanto escoger en base a la intuición y empíricamente entre aquellas que generen mejor ajuste y/o capacidad de pronóstico. De esta forma, un buen modelo debe balancear adecuadamente el uso de la teoría con la simpleza y flexibilidad del modelo.

Para ser convincente, un modelo estructural debe al menos (i) entregar suficiente flexibilidad para aprender de la data, (ii) derivar las ecuaciones de comportamiento de supuestos razonables respecto de los agentes involucrados y (iii) incorporar explícitamente en la descripción la naturaleza no experimental de la data.

**Observación:** En la discusión se ha hecho la distinción entre modelos probabilísticos y modelos estructurales. Aunque los modelos probabilísticos proveen una historia de comportamiento de los agentes, los supuestos básicos usados para derivarlos no se sustentan en ninguna teoría de comportamiento. Por ejemplo, en modelos de duración en tiempo discreto se suele suponer que los clientes dejan de estar activos con cierta probabilidad. Más que una teoría de comportamiento esto es simplemente una descripción probabilística de un fenómeno. En determinadas situaciones, especialmente en casos en que no se dispone de una descripción rica del ambiente en que los agentes toman sus decisiones, se conforma con esta descripción agregada del comportamiento. El enfoque estructural sobre el que se ahondará en esta parte resulta particularmente útil cuando se tiene suficiente información para investigar las motivaciones profundas de las elecciones. Al definir un modelo estructural, tanto las teorías de comportamiento como la descripción probabilística del sistema son fuentes válidas de estructura. Sin embargo, se considerará como modelo econométrico estructural a aquellos que se nutren de ambas fuentes.

### Modelos Estructurales en Marketing

El desarrollo de modelos estructurales se ha gestado en varias áreas del conocimiento tales como economía, transportes, logística, finanzas y marketing. Entre estas áreas, la del marketing se ha constituido en un terreno particularmente fértil para el desarrollo y adopción del enfoque estructural. Se identifican al menos cuatro motivos por los cuales la adición de estructura en los modelos econométricos es particularmente útil para el análisis de problemas comerciales:

1. *Disponibilidad de datos*. Gran parte de los datos que registran las compañías dan cuenta de las interacciones entre clientes y firma como son ocasiones de compra, visitas a sitios web corporativos o llamadas a los centros de llamadas. De esta forma, un conjunto importante de los datos disponibles dentro de las organizaciones son informativos respecto a procesos claves de la función comercial. Así, los requerimientos de datos impuestos por los modelos estructurales están inmediatamente satisfechos por procesos operacionales.

2. *Atractivo de la evaluación de la intervención de sistemas*. En la función comercial, casi por definición se busca perturbar los sistemas para mejorar la oferta de valor cambiando precios, proponiendo nuevos diseños de productos, redefiniendo la cadena logística, etc. De esta forma se necesita disponer de modelos que describan la reacción de los consumidores ante dichos cambios del ambiente competitivo lo que, de acuerdo a la crítica de Lucas, solo puede hacerse con un modelo estructural.

3. *Importancia de heterogeneidad*. En marketing se busca hacer inferencia desagregada a nivel de cliente o segmento para poder diseñar versiones especializadas del marketing mix que sean atractivas para segmentos específicos de clientes. Como los modelos estructurales requieren especificar los supuestos de comportamiento a nivel individual, la generación de estimaciones desagregadas suele derivarse directamente.

4. *Pragmatismo en la aceptación de teorías*. Como se ha argumentado, una de las ventajas de los modelos estructurales es que permiten testear si una determinada teoría de comportamiento aplica a una situación. A diferencia de otras disciplinas, en marketing hay una tradición de revisión continua de las fuerzas que moldean el comportamiento de las personas y por tanto el enfoque de modelos estructurales entrega una herramienta alternativa a la verificación experimental de nuevas teorías.

### Taxonomía de Modelos Estructurales

Metodológicamente, es útil generar una clasificación de los tipos de modelos estructurales existentes en la literatura. Como se ha consignado, uno de los costos de la inclusión de teoría en modelos econométricos es la mayor complejidad en las rutinas de estimación. Es esta complejidad la que dificulta la generación de un mecanismo único que permita estimar modelos generales y por tanto se ve forzado a usar metodologías específicas dependiendo de la naturaleza del problema. En la discusión se basará la clasificación en la evaluación de cuatro factores.

1. *Nivel de agregación de los datos*. Se ha propuesto que un modelo estructural debe basarse en una descripción detallada de los supuestos de los tomadores de decisión a nivel individual. Por lo tanto, la disponibilidad de datos a nivel individual como la decisión de compra de cada uno de los individuos de un panel de consumidores habilita para, imponiendo las restricciones de identificación necesarias, estimar los parámetros de comportamiento de manera más o menos directa. Sin embargo, en ciertas situaciones solo se dispone de información agregada, como participaciones de mercado o datos agregados de venta. En estos casos, la identificación de parámetros de comportamiento requiere además de una descripción del mecanismo mediante el cual se agregan las decisiones individuales. Este mecanismo típicamente considera la especificación de un modelo de heterogeneidad describiendo como se distribuyen los parámetros entre los clientes la que se integra sobre la población para generar las métricas agregadas. Esto es precisamente lo propuesto por el método BLP (a partir de
Berry, Levinsohn y Pakes, quienes primero propusieron el método en 1995) que describe un método que, basado en un modelo logit, permite estimar ofertas y demandas de un modelo oligopólico con información agregada. Por simplicidad, en esta versión se concentrará en modelos estimables directamente sobre datos desagregados a nivel individual.

2. *Temporalidad de las decisiones*. Dependiendo de la amplitud temporal considerada por los agentes al evaluar las alternativas de decisión, se distingue entre problemas estáticos y dinámicos. Básicamente, si se considera que las acciones que se observan resultan de una evaluación completa del horizonte, entonces se habla de problemas dinámicos. En caso contrario, se dice que el problema es estático. La distinción es importante desde un punto de vista metodológico. Si el tomador de decisiones basa sus decisiones exclusivamente
mirando el pasado, entonces estas decisiones pueden caracterizarse directamente mediante condiciones de optimidad sencillas. Por el contrario, si el tomador de decisión además evalúa las repercusiones (inciertas) que sus acciones de hoy podrían tener en su bienestar futuro, entonces se necesita caracterizar las políticas óptimas a través de ecuaciones de Bellman que incorporen explícitamente la naturaleza multiperiodo del problema. En este caso, para encontrar la política óptima del problema se requiere usar técnicas como programación dinámica estocástica o control óptimo, aumentando de manera importante la complejidad computacional de la estimación.

3. *Naturaleza de las variables de decisión*. Si las variables sobre las que deciden los agentes son continuas (gasto, montos de inversión, unidades compradas, etc.), se habla de un modelo de decisión continuo. Si las variables sobre las que deciden los agentes son discretas (si visita o no visita la tienda, si elige la marca A o marca B, etc.), se habla de un modelo de decisión discreto. La distinción es relevante en cuanto las soluciones de un problema de decisión continua pueden caracterizarse directamente mediante condiciones de Karush-Kuhn-Tucker, mientras que las soluciones de un problema de decisión discreta requieren una enumeración del valor de las alternativas.

4. *Identidad de los agentes*. Los modelos estructurales pueden usarse para estudiar tanto el comportamiento de los clientes como de las otras firmas en el mercado. El área que estudia el comportamiento de las firmas ha tenido un gran desarrollo en los últimos años y se conoce como organización industrial empírica. En esta versión, se concentrará la discusión en el estudio de los clientes por dos motivos principales: la disponibilidad de datos de comportamiento de cliente y la simpleza de las nociones de equilibrio requeridas para describir a los clientes. Mientras cada cliente suele tener poco poder de mercado por si mismo, las acciones de marketing de las firmas competidoras típicamente pueden modificar de manera importante las condiciones del mercado. Así, la descripción de las decisiones de las firmas conlleva desafíos metodológicos importantes como la inclusión de nociones sofisticadas de equilibrio para internalizar que las decisiones de las firmas resultan tanto de mirar las respuestas esperadas de los clientes como las reacciones estratégicas de los competidores.

Metodológicamente es útil también distinguir los métodos de estimación de los modelos. La literatura reconoce dos grandes enfoques para estimar modelos estructurales como los aquí presentados: método de los momentos generalizados (GMM) y método de la máxima verosimilitud. Dada su eficiencia estadística (en el sentido que usa toda la información disponible), en esta primera versión se usará solo el método de la máxima verosimilitud. En lo que sigue se enfocará la discusión al estudio del comportamiento de clientes, en problemas estáticos (o con dinámica limitada a la incorporación del pasado) y con datos desagregados. Partiremos describiendo brevemente modelos de decisión continuos para luego iniciar una discusión más extensa en modelos de decisión discreta que tienen una tradición más larga en marketing.



## Logit

### Modelos de Elección Discreta

Un modelo de elección discreta consiste básicamente en situaciones en que la naturaleza de las variables de decisión a las que se enfrenta el tomador de decisión son discretas. Para ilustrar la intuición de la diferencia con respecto a modelos de decisión continua, es útil pensar que mientras estos últimos buscan describir decisiones de "el cuánto", los modelos de elección discreta se concentran en "el cuál". La distinción además relevante desde un punto de vista metodológico. A diferencia de los modelos de elección continua en que la optimidad de la elección queda bien descrita por
condiciones de primer orden, al enfrentar decisiones discretas caracterizaremos la optimidad por enumeración. Ejemplos típicos en que la decisión a evaluar es de naturaleza discreta incluye la elección de una marca por sobre otra en la góndola de un supermercado, la decisión de visitar o no a una tienda, la elección del color de una prenda de vestir, de un canal de venta y la elección de las firmas respecto a entrar o no entrar a un mercado.

Para que un problema de elección discreta esté bien definido, se necesita, además de variables de decisión discretas, que el conjunto de alternativas presente las siguientes tres características:

1. *EXHAUSTIVAS*:  El conjunto sobre el que los tomadores de decisión eligen deben incluir todas las alternativas posibles. En otras palabras, cualquiera sea la decisión observada, debe
estar incluida en el conjunto de elección. Esta condición es poco restrictiva ya que siempre es posible incluir en el set de alternativas la posibilidad “ninguna de las anteriores” o similar que por definición incluya toda las otras posibilidades no consideradas en conjunto. Sin embargo, esta estrategia debe usarse con precaución. Por ejemplo, al estudiar la elección de marca en una categoría en que se observa que los clientes no siempre compran alguna
de las marcas disponibles, se podría incluir la alternativa de no compra en el conjunto de elección. Si la proporción de no compras es alta en la muestra, la inclusión de la alternativa de no compra podría limitar la habilidad del modelo de aprender respecto a cómo los clientes eligen entre marcas. En este caso, podría convenir concentrarse en la elección de la marca condicional en haber hecho una compra en la categoría.

2. *MUTUAMENTE EXCLUYENTES:* El conjunto de decisión debe definirse de modo que en cada ocasión el tomador de decisión seleccione solo una de las alternativas disponibles. Esto es, la elección de una alternativa implica necesariamente la no elección de cualquiera de las alternativas restantes. Aunque aparentemente restrictiva, la definición de conjunto de elección puede acomodarse para generar conjuntos mutuamente excluyentes. Por ejemplo, considérese un modelo para describir la elección de los clientes entre la *tienda física tradicional* o la *tienda virtual*. Si simplemente se permite una alternativa de elección por cada canal, entonces se excluye la posibilidad de que un mismo cliente esté en más de un canal al mismo tiempo. Para incorporar esta posibilidad, se debe redefinir las alternativas agregando la opción de *tienda tradicional y virtual*.

3. *FINITO:* El conjunto de decisión debe contener un conjunto finito de alternativas. Esta condición es importante por dos motivos técnicos. Primero, un conjunto finito facilita la evaluación de la optimidad de las decisiones y, segundo, facilita la definición de probabilidades de elección. Existen situaciones en que la decisión teóricamente permite infinitas posibilidades, pero que en la práctica se concentran en un número reducido de alternativas y, por tanto, quedan bien representadas por un modelo de elección discreta. Por ejemplo, se puede usar el número de cajas de cereal compradas por los clientes en cada visita al supermercado. Aunque teóricamente los clientes siempre podrían comprar una unidad adicional, el problema queda bien descrito considerando sólo las alternativas de 0, 1, 2, 3 o más de 3 cajas. 

Un modelo estructural para describir la probabilidad de elegir cada alternativa necesita especificar el mecanismo que usan los agentes para decidir entre las alternativas. Se partirá asumiendo que, en cada oportunidad de compra $t$, el tomador de decisión $n$ elige la alternativa $i$ que le reporta mayor utilidad $u_{nit}$. Pese a que el tomador de decisión necesita conocer la utilidad que deriva de cada una de las alternativas, desde la perspectiva del analista solo se observan algunas características del ambiente de decisión y del tomador de decisión a partir de las cuales se puede intentar aproximar la utilidad del tomador de decisión a través de una función $v_{nit}(x_{nit}, \theta)$ donde $x_{nit}$ son las características observables del problema y $\theta$ el vector de parámetros que se busca estimar y que describen la relación de dichas características con la utilidad.

**Ejemplo:** Supóngase que se quiere describir la elección del medio de pago que usan los usuarios de una tienda determinada, la que permite pagar en efectivo o con alguna tarjeta bancaria. El analista observa 3 variables que intuye pueden ser relevantes en la elección del medio de pago: el género del cliente ($F_n = 1$ si cliente es de género femenino), su nivel de ingresos $(I_n)$ y el monto de la transacción $(M_{nt})$. Son precisamente estas características las que estarían incluidas en la matriz que se ha llamado $x_{nit}$. A partir de esta información pueden plantearse múltiples modelos para describir $v_{nit}$ (se asumirá que $i = 0$ corresponde al caso de pago con efectivo mientras que $i = 1$ al de pago con tarjeta).

* *Modelo Lineal Homogéneo*: Aquí, la utilidad para ambas alternativas crece linealmente con las variables observables. En este caso, los parámetros son los mismos para todos los tomadores de decisión y por tanto el vector de parámetros viene dado por $\theta = (\alpha_0, \alpha_1, \beta, \gamma, \delta)$

$$v_{nit} = \alpha_i + \beta F_n + \gamma I_n + \delta M_{nt}$$

* *Modelo lineal heterogéneo:* Aquí, la utilidad para ambas alternativas también crece linealmente con las variables observables, pero ahora los parámetros varían por alternativa y por agente, y por tanto el vector de parámetros viene dado por $\theta = ( \{ \alpha_{1n} \} _{n=1}^{N}, \beta_0,\beta_1,\gamma_0,\gamma_1,\{ \delta_n \}_{n=1}^{N} )$

 $$v_{nit} = \alpha_{in} + \beta_i F_n + \gamma_i I_n + \delta_n M_{nt}$$
 
La definición de que los interceptos dependen del cliente $n$ simplemente indica que cada cliente tiene una preferencia intrínseca por cada medio de pago. Del mismo modo, se está imponiendo que la influencia que tiene el monto en el atractivo que tiene cada alternativa depende del cliente. Por ejemplo, mientras para algunos clientes el monto de la transacción puede jugar un rol importante en la decisión del medio de pago, para otros este efecto podría no ser relevante. Por último, la dependencia de la alternativa en los parámetros asociados a género e ingreso podrían usarse para por ejemplo situaciones en que el nivel de ingreso afecta el atractivo de un medio de pago pero no del otro (la intuición para el género es análoga).

Por supuesto, también se pueden postular modelos no lineales u otras especificaciones de la heterogeneidad. Por ejemplo, que la influencia del ingreso varíe por medio de pago, pero que
el efecto del género sea constante entre las alternativas. Descubrir la especificación que mejor describe el problema es precisamente la tarea del analista.

**Observación:** En el ejemplo se ha introducido brevemente el concepto de heterogeneidad. Sin embargo, para facilitar la exposición de los temas básicos, en primera instancia se concentrará en modelos sin heterogeneidad. En marketing, los modelos que incluyen heterogeneidad en las preferencias son tan importantes que se postergará su discusión en un capítulo separado.

En la práctica, aún en situaciones en que se observa con detalle el ambiente de decisión, no se podrá describir con exactitud todos los factores que gobiernan el comportamiento de los agentes. Por lo tanto, se definirá $\varepsilon_{nit}$ como el error (aditivo) que se comete al aproximar $u_{nit}$ a través de $v_{nit}$.

$$u_{nit} = v_{nit} + \varepsilon_{nit}$$
Así, se descompone la utilidad de cada alternativa en una componente sistemática (u observable o explicable) $v_{nit}$ y en una componente aleatoria (o no observable o inexplicable) $\varepsilon_{nit}$. Como se verá, la tarea de modelamiento del problema involucra tanto la especificación de la componente sistemática como de la aleatoria.

La componente básica para estimar estadísticamente un modelo de elección discreta es la especificación de la probabilidad de elección de cada alternativa. Sea $P_{nit}$ la probabilidad de que el agente $n$ escoja la alternativa $i$ en la oportunidad de compra $t$. El supuesto de maximización de utilidades implica que $P_{nit}$ puede escribirse como:

\begin{aligned}
P_{nit} &= Pr(u_{nit}>u_{njt},\forall j \neq i)\\
&= Pr(v_{nit} + \varepsilon_{nit} >v_{njt} + \varepsilon_{njt} ,\forall j \neq i)\\
&= \int \textbf{1} (\varepsilon_{njt} - \varepsilon_{nit} > v_{nit} - v_{njt}) f(\varepsilon_{nt}) d \varepsilon_{nt}
\end{aligned}

donde $\textbf{1}(\cdot)$ toma el valor 1 si se cumple el argumento y el valor 0 en caso contrario. En esta expresión, $\varepsilon_{nt} = (\varepsilon_{n1t},\varepsilon_{n2t}, ...,\varepsilon_{nIt})$ es el vector de las componentes aleatorias de la elección del agente $n$ en la oportunidad $t$, y $f(·)$ la función de densidad que describe su comportamiento probabilístico. La elección de la distribución de la componente aleatoria es importante en cuanto impone restricciones a los patrones de comportamiento que pueden ser capturados por el modelo. Se concentrará la atención en los casos en que $\varepsilon_{nit}$ se distribuye valor extremo, que da origen al modelo *logit*, y normal, que da origen al modelo *probit*.

### Modelo Logit

El modelo logit resulta de asumir que cada $\varepsilon_{nit}$ es independientemente distribuido de acuerdo a una distribución gumbel o de valor extremo tipo I.

\begin{equation} 
  \begin{array}{cc}
  F(\varepsilon_{nit}) = e^{-e^{-\varepsilon_{nit}}}
   &f(\varepsilon_{nit}) = e^{-\varepsilon_{nit}}e^{-e^{-\varepsilon_{nit}}}
   \end{array}
  (\#eq:logituno)
\end{equation} 

Aplicando esta definición, se puede demostrar que la probabilidad de elección en un modelo logit corresponde a una fórmula cerrada sencilla (para el detalle de la derivación ver [apéndice](#apéndice)):

\begin{equation}
\begin{aligned}
P_{nit} &= \int Pr(\varepsilon_{njt} < v_{nit} - v_{njt} + \varepsilon_{nit}, \forall j \neq i | \varepsilon_{nit}) f(\varepsilon_{nit})d\varepsilon_{nit}\\
&= \int \left(\prod_{j \neq i} e^{-e^{-(v_{nit} - v_{njt} + \varepsilon_{nit})}}\right) e^{-\varepsilon_{nit}}e^{-e^{-\varepsilon_{nit}}}d \varepsilon_{nit} \\
&= \frac{e^{v_{nit}}}{\sum_j e^{v_{njt}}}
\end{aligned}
(\#eq:logitprob)
\end{equation}

En algunos libros de texto se justifica esta expresión simplemente como una regresión logística, esto es, una transformación lineal para normalizar la utilidad de modo de interpretarla directamente como una probabilidad de elección en el rango [0,1]. Aunque válido, resulta útil entender que, en efecto, dicha expresión puede derivarse a partir de supuestos de maximización de utilidades.

Para ganar algo de intuición respecto a la expresión de la probabilidad de elección, es útil graficarla con respecto a la utilidad derivada por cada alternativa. Por ejemplo, supóngase que se tiene una decisión binaria que, por ejemplo, corresponde a la decisión de comprar o no comprar un producto. En este caso, la probabilidad de comprar el producto crece *sigmoidalmente* con la utilidad derivada de la compra. Esto es, al graficar la probabilidad de compra con respecto a la utilidad derivada, se obtiene una curva S como muestra la Figura 1. En la figura, se ha agregado también la curva de la probabilidad de elección en el caso en que, en vez de asumir que el error se distribuye valor extremo como demanda el modelo logit, se asume que el error está normalmente distribuido como tradicionalmente se hace en otros modelos econométricos.

```{r probelec, fig.cap="Probabilidad de elección",out.width='50%', fig.align='center'}
knitr::include_graphics(rep("images/probabilidad_eleccion.png"))
```

\FloatBarrier

La disposición de una fórmula cerrada para la probabilidad de elección facilita el cálculo de múltiples métricas asociadas que permiten complementar el análisis. Supóngase que la utilidad de una alternativa viene dada por $v_{nit} = v(x_{nit}, \theta)$; entonces se pueden calcular:

+ Cómo varía la probabilidad de elegir la alternativa $i$ al variar alguna componente de la utilidad de la misma alternativa.

\begin{equation}
\frac{dP_{nit}}{dx_{nit}}  = \frac{\partial v_{nit}}{\partial x_{nit}} \cdot P_{nit}(1- P_{nit})
(\#eq:ownderivative)
\end{equation}

+ Cómo varía la probabilidad de elegir la alternativa $i$ al variar alguna componente de la utilidad de otra alternativa.

\begin{equation}
\frac{dP_{nit}}{dx_{njt}}  = \frac{\partial v_{njt}}{\partial x_{njt}} \cdot P_{nit}\cdot P_{njt}
(\#eq:crossderivative)
\end{equation}

+ Elasticidad de la probabilidad de elegir la alternativa $i$ con respecto a alguna componente de la utilidad de la misma alternativa.

\begin{equation}
e_{ix_{nit}} = \frac{\partial P_{nit}}{\partial x_{nit}}  \cdot \frac{x_{nit}}{P_{nit}}= \frac{\partial v_{nit}}{\partial x_{nit}} x_{nit} (1- P_{nit})
(\#eq:ownelasticity)
\end{equation}

+ Elasticidad de la probabilidad de elegir la alternativa $i$ con respecto a alguna componente de la utilidad de otra alternativa.

\begin{equation}
e_{ix_{njt}} = \frac{\partial P_{nit}}{\partial x_{njt}}  \cdot \frac{x_{njt}}{P_{nit}}= \frac{\partial v_{njt}}{\partial x_{njt}} x_{njt} P_{njt}
(\#eq:crosselasticity)
\end{equation}

Recuerde que una de las motivaciones para el uso de modelos estructurales es la posibilidad de analizar contrafactuales, esto es, ver qué pasaría con el mercado si hay cambio en alguna variable de control interesante. Por ejemplo que pasa con las participaciones de mercado si sube el precio de una alternativa, si se aumenta la frecuencia publicitaria, etc. Las métricas recién presentadas permiten precisamente hacer dichas evaluaciones de manera directa.

#### Propiedades del modelo Logit

El modelo logit es bastante flexible para acomodar una amplia variedad de situaciones. En efecto, distintas especificaciones de las funciones de utilidades de las alternativas permiten describir múltiples fenómenos asociados a la elección. Sin embargo, es importante reconocer que los supuestos subyacentes al logit imponen importantes restricciones a cómo se describe la lógica en que los agentes evalúan las alternativas y escogen entre ellas.

Para fijar ideas, resulta útil pensar qué restricciones impone asumir que las componentes no observables de la utilidad son todas independientes entre ellas. El supuesto de independencia obliga a imponer que cualquier relación entre las utilidades de dos alternativas debe necesariamente capturarse a través de variables observables. Del mismo modo, las utilidades que se derivan por dos alternativas en ocasiones de elección diferentes solo pueden describirse a través de elementos que se puedan observar a lo largo del tiempo. Para entender mejor cómo estas limitaciones se materializan en la formulación del modelo, se discutirán formalmente tres características del modelo logit: la existencia de patrones de sustitución proporcional, la incapacidad de capturar tanto heterogeneidad aleatoria en las preferencias como componentes dinámicas no observables.

##### Patrones de sustitución {-}

Los patrones de sustitución derivados de un modelo logit son bastante peculiares y, aunque desde un punto de vista econométrico puede resultar beneficioso, desde el punto de vista de la investigación de teorías de comportamiento suele ser considerado como bastante restrictivo. Se entenderá por patrones de sustitución a la forma en que cambia la probabilidad de elección de alguna alternativa cuando se modifica el atractivo de otra alternativa. Para entender la naturaleza de los patrones de sustitución del modelo logit es útil calcular el ratio de las probabilidades de elección de dos alternativas cualquiera $i$ y $j$.

$$\frac{P_{ni}}{P_{nj}} = e^{v_{ni} - v_{nj}}$$

Este ratio solo depende de las utilidades observables de las dos alternativas consideradas lo que indica que la probabilidad relativa de elegir la alternativa $i$ sobre la alternativa $j$ no depende de que otras alternativas existan ni de los atributos que ellas tengan. Por ejemplo, si se agrega una alternativa al conjunto de elección, el ratio de probabilidades de las alternativas existentes se mantendrá constante independiente de las características de la nueva alternativa. Se referirá a esta característica como *independencia de alternativas irrelevantes* o *IIA*.

Para ejemplificar, considérese una botillería que ofrece dos variedades de vino, uno blanco y otro tinto. Supóngase además que estas dos alternativas tienen la misma participación de
mercado, esto es la mitad de los clientes de la botillería compra vino blanco y la otra mitad compra vino tinto. En este caso, las utilidades sistemáticas debieran ser similares y, por tanto, el ratio de probabilidades de elección de vino blanco sobre vino tinto debiera acercarse a 1. Motivado por un
mayor margen de los vinos tintos, el administrador de la botillería decide incorporar una nueva variedad de vino tinto. Intuitivamente se esperaría que, como la nueva variedad de vino tinto es un sustituto más cercano al tinto existente, la participación de mercado de este debiera decrecer
más que la de vino blanco. Sin embargo, la propiedad de IIA impone que este ratio se mantiene constante. En otras palabras, la introducción de una nueva alternativa disminuirá la participación de todas la otras alternativas independiente de las similitudes que tengan. Esta última observación
puede corroborarse calculando la elasticidad de sustitución $E_{ix_{nj}}$ que determina como cambia la probabilidad de consumir la alternativa $i$ ante un cambio en un atributo $x_{nj}$ de la alternativa $j$. 

$$E_{ix_{nj}} = -\frac{\partial v_{nj}}{\partial x_{nj}}x_{nj} P_{nj}, \forall i \neq j$$
 
Esta expresión no depende de $i$, por lo que es constante para todas las alternativas de elección. Luego, si ocurre una mejora en los atributos de una alternativa, la probabilidad de elección de las demás disminuye en el mismo porcentaje independiente de la similitud entre alternativas. Se referirá a esta característica como *patrones de sustitución proporcionales*.

Una ventaja de los patrones de sustitución del modelo logit es que permite que los parámetros del modelo sean estimados consistentemente en base a un subconjunto de las alternativas. Esto es particularmente útil en ambientes de decisión de marketing donde típicamente se encuentran centenas de productos que potencialmente pueden constituir alternativas de elección en una situación de compra. De esta forma, para estimar un modelo logit se pueden seleccionar conjuntos
reducidos de alternativas que capturan los elementos esenciales de la elección e ignorar qué pasa con todas las otras alternativas.

##### Incapacidad de estimar componentes aleatorias {-}

La investigación de las diferencias entre las preferencias de los distintos clientes es un tema fundamental para el desarrollo de planes comerciales exitosos. Tradicionalmente se distinguen dos tipos de heterogeneidad de acuerdo a la capacidad de observación del analista. En un principio, se tiene el estudio de heterogeneidad observable que indica cómo las preferencias de los tomadores de decisiones varían de acuerdo a sus características medibles. Este tipo de heterogeneidad permite, por ejemplo, estudiar diferencias en las preferencias entre hombres y mujeres, por edad o por niveles de ingreso. Sin embargo, una proporción importante de las diferencias de las preferencias no es atribuible a características observables como las recién descritas. Otro ejemplo es que dos hermanos del mismo género, de edades similares, viviendo en el mismo hogar, pueden tener preferencias completamente diferentes respecto a sabores de yogur.

El resultado fundamental en esta sección indica que un modelo logit permite estudiar variaciones de preferencias asociadas a componentes observables, pero no a componentes no observables.
Para ilustrar este resultado, supóngase un tomador de decisión caracterizado por la siguiente función de utilidad:

$$u_{nit} = \alpha_i + \beta_np_{it} + \varepsilon_{nit}$$
 
 Es decir, la utilidad de cada alternativa tiene una componente base que es constante entre los tomadores de decisión y una penalización por precio $p_{it}$ al que se enfrenta el tomador de decisión. Al indexar $β_n$ por agente se está explícitamente permitiendo que algunos tomadores de decisión sean más sensibles al precio que otros. Supóngase que se postula que el coeficiente de precio viene dado por la siguiente ecuación de regresión.
 
 $$\beta_n = \lambda_0 + \lambda_1I_n + \mu_n$$
 Donde $\lambda_0$ captura la sensibilidad base al precio, $I_n$ el nivel de ingreso del agente $n$ y $\lambda_1$ el coeficiente que indica cómo dichos niveles de ingresos afectan la sensibilidad al precio. Por último, $\mu_n$ es un valor aleatorio que captura todas las otras componentes que modifican la sensibilidad al precio más allá del nivel base y los ingresos.
 
\begin{aligned}
u_{nit} &= \alpha_i + (\lambda_0 + \lambda_1 I_n + \mu_n)p_{it} + \varepsilon_{nit}\\
&= \alpha_i + \lambda_0 p_{it} + \lambda_1p_{it}I_n + \xi_{nit}
\end{aligned}
 
 
 Donde $\xi_{nit} = \mu_np_{it} + \varepsilon_{nit}$. De esta expresión debiera ser claro que la inclusión de heterogeneidad
observable puede ser capturada bajo un enfoque logit. En efecto, los parámetros $\alpha_i$, $\lambda_0$ y $\lambda_1$ dan cuenta respectivamente del nivel de utilidad base por alternativa, de la penalización por precio y de como dicha penalización se ve modificada por el nivel de ingresos. Lamentablemente, la
variación aleatoria $\mu_n$ no puede ser incluida, ya que su inclusión necesariamente implica que las componentes errores $\xi_{nit}$ no están idénticamente distribuidas. En efecto, se puede mostrar que $\mathbb{V}ar(\xi_{nit}, \xi_{njt}) = \mathbb{Var}(\mu_n)p^2_{it}$ que evidentemente varía entre alternativas. Más aún, también se puede mostrar que $\mathbb{Cov}(\xi_{nit}, \xi_{njt}) = \mathbb{Var}(\mu_n)p_{it}p_{jt} \neq 0$, violando también el supuesto de independencia.

 Es importante notar que la incapacidad de capturar aleatoriedad aplica también a componentes dinámicas. Esto es, al observar compras repetidas en el tiempo, el modelo logit no permite capturar que hay componentes no observables que varíen en el tiempo. Por ejemplo, no se puede incorporar que, debido a factores externos no observables, en algunos períodos algunas alternativas son más atractivas para todos los agentes decidiendo en dichos períodos. Al igual que en el ejemplo anterior, incluir estas variaciones viola los supuestos de distribuciones independientes e idénticamente distribuidas para las componentes no observables.
 
### Estimación

Para estimar el modelo, se necesita escribir la verosimilitud del problema. La componente fundamental para la construcción de la verosimilitud es la descripción de la probabilidad de
elección $P_{nit}$. Para el caso del modelo logit, como la expresión de la probabilidad de elección corresponde a una fórmula analítica cerrada, la construcción de la verosimilitud es directa. Si la componente determinística de la utilidad viene dada por $v_{nit}(x_{nit}, \theta)$ y si $y_{nit}$ es una variable que toma valor 1 si el tomador de decisión $n$ escoge alternativa $i$ en oportunidad $t$, entonces la verosimilitud viene dada por:

\begin{equation}
L(\theta) = \prod_n\prod_i\prod_t (P_{nit})^{y_{nit}} =  \prod_n\prod_i\prod_t \left(\frac{e^{v_{nit}(x_{nit},\theta)}}{\sum_j e^{v_{njt}(x_{njt},\theta)}}\right)^{y_{nit}}
(\#eq:likelihood)
\end{equation}

La que se puede maximizar directamente usando rutinas estándares de programación convexa. Computacionalmente, suele ser más conveniente trabajar con la log-verosimilitud en vez de la verosimilitud. Esto porque la multiplicación de probabilidades genera muy rápidamente valores que computacionalmente son indistinguibles de cero. Recuérdese que el valor de los valores óptimos son invariantes a transformaciones monótonas como la del logaritmo. 

\begin{equation}
\begin{aligned}
LL(\theta) &= \sum_n\sum_i\sum_t y_{nit} ln \left(\frac{e^{v_{nit}(x_{nit},\theta)}}{\sum_j e^{v_{njt}(x_{njt},\theta)}}\right)\\
&= \sum_n\sum_i\sum_t y_{nit}v_{nit}(x_{nit},\theta) - \sum_n\sum_i\sum_t ln \left(\sum_j e^{v_{njt}(x_{njt},\theta)}\right)
\end{aligned}
(\#eq:loglikelihood)
\end{equation}

Como se ha indicado, esta función objetivo puede ser ingresada directamente a cualquier rutina de optimización para encontrar los estimadores máximo verosímiles. Además, para muchas instancias prácticas, es conveniente contar con las derivadas de la log-verosimilitud, de modo de encontrar eficientemente direcciones de máximo ascenso o evaluar si el punto es estacionario o no. Afortunadamente, para la mayoría de las especificaciones del modelo logit, estas derivadas también son fáciles de obtener. Por ejemplo, si la componente sistemática de la utilidad viene dada por $v_{nit}(x_{nit}, \theta) = x'_{nit}\theta$ entonces

\begin{equation}
\frac{\partial LL(\theta)}{\partial \theta} = \sum_n\sum_i\sum_t \left(y_{nit} - \frac{e^{x'_{nit}\theta}}{\sum_j e^{x'_{njt}\theta}}\right)x_{nit}
(\#eq:gradient)
\end{equation}

Del mismo modo, se pueden calcular segundas derivadas que resultan útiles para el cálculo de errores estándares de los parámetros.

#### Evaluación del modelo {-}

Al igual que en otros modelos econométricos, una de las componentes fundamentales del análisis es la evaluación de la calidad del modelo. La variedad de métricas disponibles para la evaluación es muy amplia y la mayoría son transversales a cualquier modelo. Categorizaremos las herramientas de evaluación en tres grupos: bondad de ajuste, capacidad de pronóstico y test de hipótesis.

1. *BONDAD DE AJUSTE*: Las métricas de bondad de ajuste básicamente nos indican qué tan bien el modelo ajusta a la data. En el contexto de modelos de regresión, se suele analizar el estadístico $R^2$ que mide la proporción de la variabilidad de la variable dependiente que puede ser explicado por la variación de las variables independientes. En el contexto de modelos de elección discreta se basará la evaluación en el valor de la verosimilitud usando alguno o varios de los siguientes indicadores:

   - $\rho$ de McFadden. Este índice está en el rango [0,1] e informalmente, se suele interpretar como el coeficiente de determinación $(R^2)$ en el sentido que un valor cercano a 0 indica un mal ajuste y un valor cercano a 1 indica un buen ajuste. Sin embargo, es importante notar que no puede decirse que $\rho$ mida la variabilidad explicada por el modelo como
hace el coeficiente de determinación

    \begin{equation}
    \rho=1 - \frac{LL(\hat{\beta})}{LL(0)}
    (\#eq:rho)
    \end{equation}

  - Criterio de información de Akaike (AIC) y Bayesiano (BIC): Una de las limitaciones del $\rho$ de McFadden es que solo permite comparar modelos con el mismo número de parámetros. Los dos indicadores más usados para comparar modelos con distintos números de parámetros son AIC y BIC, en que se penaliza la verosimilitud por el número de parámetros para capturar el hecho de que al incluir nuevos parámetros la verosimilitud necesariamente crecerá. La diferencia entre AIC y BIC es que el primero tiene una penalización constante por número de parámetros mientras que la penalización del segundo depende de la cantidad de datos disponible. Si la log-verosimilitud de un modelo con n observaciones y k parámetros , entonces AIC y BIC vienen dados por: 
  
\begin{equation}
\begin{array}{cc}
AIC = -LL(\hat{\theta}) + 2k & BIC =-2LL(\hat{\theta}) + k\ln(n)
\end{array}
(\#eq:aicbic)
\end{equation}

1. *CAPACIDAD DE PRONÓSTICO*: Un modelo que explique muy bien la data puede correr el riesgo de sobreajustar. Esto es, que no permita describir el fenómeno para datos que no se usaron en el entrenamiento del modelo. Para medir la capacidad de pronóstico se suele dividir los datos en un subconjunto de entrenamiento y otro de prueba. Con esto, se comparan las realizaciones con lo pronosticado usando las estimaciones del subconjunto de entrenamiento. Supóngase que se está interesado en evaluar la capacidad de pronóstico de un indicador $f_{ni}$ que puede corresponder a las elecciones mismas, participaciones de mercado o cualquier otra. Si $\hat{f}_{ni}$ es el pronóstico del modelo, entonces se suele usar el mean
absolute error (MAE) o el mean absolute percentage error (MAPE) 

\begin{equation}
\begin{array}{cc}
MAE = \frac{1}{N} \sum_n \sum_i |f_{ni} - \hat{f}_{ni}| &MAPE = \frac{1}{N} \sum_n \sum_i \left| \frac{f_{ni} - \hat{f}_{ni}}{f_{ni}} \right|
\end{array}
(\#eq:maemape)
\end{equation}

3. *TEST DE HIPÓTESIS*: La evaluación de hipótesis también puede contribuir a diagnosticar un modelo. Por ejemplo, al agregar una variable explicativa, se podría evaluar si el coeficiente correspondiente es significativamente diferente de 0, lo que se puede hacer directamente a través de la construcción de intervalos de confianza o su estadístico $t$ equivalente (recuérdese que la varianza del estimador máximo verosímil puede obtenerse usando el inverso del Hessiano). En ocasiones también se estará interesado en testear hipótesis más complejas, para lo que se recurre a test de ratios de verosimilitud. Supóngase, por ejemplo, que se tiene un modelo en que los coeficientes asociados a display difieren por marca para incorporar la posibilidad de que algunas de ellas sean más efectivas en su comunicación en sala. El test de ratios de verosimilitud permite, por ejemplo, testear si estos coeficientes son iguales o si efectivamente difieren entre marcas. Si la hipótesis nula puede expresarse como $k$ restricciones sobre los parámetros, entonces se puede estimar un modelo A no restringido y otro B restringido y se calcula el estadístico $LR = 2(LLA − LLB)$, que se distribuye $\chi^2$
con k grados de libertad.

### Modelos de Panel: Persistencia y Dependencia Temporal

Hasta ahora se han discutido modelos de elección discreta asumiendo implícitamente que cada observación de elección es independiente de las demás. Sin embargo, cuando se dispone de datos de panel (observaciones repetidas del mismo individuo a lo largo del tiempo), es razonable esperar que las elecciones de un individuo en diferentes períodos estén relacionadas. En contextos de marketing, esta dependencia temporal puede manifestarse de diversas formas, siendo dos de las más relevantes la **persistencia de elección** (o lealtad) y la formación de **precios de referencia**.

#### Persistencia de Elección (Lealtad)

Uno de los fenómenos más documentados en el comportamiento de compra es la tendencia de los consumidores a repetir sus elecciones previas, lo que comúnmente se denomina lealtad de marca o inercia en el comportamiento. Esta persistencia puede originarse en múltiples factores:

- **Costos de cambio:** Tanto físicos (esfuerzo de probar algo nuevo) como psicológicos (aversión al riesgo)
- **Aprendizaje:** Experiencias positivas pasadas reducen la incertidumbre sobre el producto
- **Satisfacción acumulada:** La utilidad experimentada en compras previas afecta la percepción actual
- **Hábito:** Rutinas de compra que simplifican el proceso de decisión

Para capturar esta persistencia en un modelo logit, se introduce una variable de **lealtad latente** $z_{nit}$ que representa la propensión no observable del individuo $n$ a elegir la alternativa $i$ en el período $t$, basada en su historia de elecciones previas.

**Modelamiento de la Persistencia:**

La utilidad del individuo $n$ por la alternativa $i$ en el período $t$ se especifica como:

\begin{equation}
u_{nit} = \alpha_i + \beta x_{it} + \gamma z_{nit} + \varepsilon_{nit}
(\#eq:utilpersist)
\end{equation}

donde:

- $\alpha_i$ es la constante alternativa-específica
- $x_{it}$ son las covariables observables (precio, promoción, etc.)
- $\beta$ es el vector de coeficientes asociados a las variables observables
- $z_{nit}$ es la **lealtad latente** (no observable)
- $\gamma$ mide el efecto de la persistencia en la elección
- $\varepsilon_{nit}$ es el término de error i.i.d. valor extremo tipo I

**Construcción de la Variable de Lealtad:**

La variable de lealtad $z_{nit}$ se construye como un promedio ponderado exponencialmente de las elecciones pasadas del individuo. Sea $y_{nit}$ una variable indicadora que toma valor 1 si el individuo $n$ eligió la alternativa $i$ en el período $t$ y 0 en caso contrario. Entonces:

\begin{equation}
z_{nit} = \lambda z_{nit-1} + (1 - \lambda) y_{nit-1}
(\#eq:loyalty)
\end{equation}

donde $0 \leq \lambda \leq 1$ es un **parámetro de decaimiento** que determina qué tan rápido se desvanece el efecto de las elecciones pasadas.
Con $\lambda = 0$ solo importa la elección inmediatamente anterior ($z_{nit} = y_{nit-1}$) y con **$\lambda = 1$:**, todas las elecciones pasadas tienen el mismo peso (memoria perfecta).

Expandiendo recursivamente la ecuación \@ref(eq:loyalty):

\begin{aligned}
z_{nit} &= \lambda z_{nit-1} + (1-\lambda) y_{nit-1}\\
&= \lambda[\lambda z_{nit-2} + (1-\lambda)y_{nit-2}] + (1-\lambda)y_{nit-1}\\
&= \lambda^2 z_{nit-2} + \lambda(1-\lambda)y_{nit-2} + (1-\lambda)y_{nit-1}\\
&= \lambda^2[\lambda z_{nit-3} + (1-\lambda)y_{nit-3}] + \lambda(1-\lambda)y_{nit-2} + (1-\lambda)y_{nit-1}\\
&= \lambda^3 z_{nit-3} + \lambda^2(1-\lambda)y_{nit-3} + \lambda(1-\lambda)y_{nit-2} + (1-\lambda)y_{nit-1}\\
&= \vdots\\
&= (1-\lambda)y_{nit-1} + \lambda(1-\lambda)y_{nit-2} + \lambda^2(1-\lambda)y_{nit-3} + \cdots + \lambda^{t-2}(1-\lambda)y_{ni1}\\
&= (1-\lambda)\sum_{s=1}^{t-1} \lambda^{t-1-s} y_{nis}
\end{aligned}

Esta expresión muestra que $z_{nit}$ es un promedio ponderado de todas las elecciones pasadas, donde las elecciones más recientes reciben mayor peso exponencialmente decreciente.

**Inicialización:**

Para el primer período observado, se requiere una condición inicial. Las opciones comunes son:

- $z_{ni1} = 0$ para todas las alternativas
- $z_{ni1} = 1/J$ donde $J$ es el número de alternativas (distribución uniforme)
- $z_{ni1}$ basado en participación de mercado agregada

**Estimación:**

El parámetro $\lambda$ puede ser:

1. **Fijado a priori:** Por ejemplo, $\lambda = 0$ para considerar solo la elección inmediata anterior
2. **Estimado como parte del modelo:** Se incluye $\lambda$ como parámetro adicional a estimar, buscando en una grilla de valores $(0, 0.1, 0.2, ..., 0.9)$ o mediante búsqueda numérica
3. **Específico por alternativa:** $\lambda_i$ diferente para cada alternativa, permitiendo que algunas marcas generen más lealtad que otras

**Interpretación de $\gamma$:**

El coeficiente $\gamma$ mide la magnitud del efecto de la persistencia:

- **$\gamma > 0$:** Existe persistencia positiva - los consumidores tienden a repetir elecciones previas
- **$\gamma = 0$:** No hay efecto de persistencia - las elecciones son independientes en el tiempo
- **$\gamma < 0$:** Búsqueda de variedad - los consumidores tienden a cambiar de alternativa

En la mayoría de contextos de marketing, se espera $\gamma > 0$, aunque en categorías donde la variedad es valorada (helados, restaurantes) podría observarse $\gamma < 0$.

**Ejemplo Numérico:**

Supóngase un mercado de yogur con 3 marcas (A, B, C) y un consumidor que en los últimos 5 períodos eligió: A, A, B, A, ? (período actual). Con $\lambda = 0.5$:

Para la marca A:
\begin{aligned}
z_{A,t} &= (1-0.5)[1 \cdot 0.5^0 + 0 \cdot 0.5^1 + 1 \cdot 0.5^2 + 1 \cdot 0.5^3]\\
&= 0.5[1 + 0 + 0.25 + 0.125] = 0.6875
\end{aligned}

Para la marca B: $z_{B,t} = 0.5[0 + 1 \cdot 0.5^1 + 0 + 0] = 0.25$

Para la marca C: $z_{C,t} = 0$

Si $\gamma = 2$, entonces la marca A recibe un incremento de utilidad de $2 \times 0.6875 = 1.375$ debido a la lealtad acumulada.

#### Precios de Referencia

Otro aspecto fundamental del comportamiento de compra con datos de panel es la formación de **precios de referencia** (reference prices). Los consumidores no evalúan los precios en términos absolutos, sino que los comparan con un precio de referencia interno formado a partir de precios observados en el pasado. Esta noción está bien fundamentada tanto en teoría económica (teoría de prospectos) como en evidencia empírica de marketing.

**Modelamiento de Precios de Referencia:**

La utilidad se especifica incorporando tanto el efecto del precio actual como de la desviación respecto al precio de referencia:

\begin{equation}
u_{nit} = \alpha_i + \beta_1 P_{nit} - \beta_2(P_{nit} - RP_{nit}) + \delta x_{nit} + \varepsilon_{nit}
(\#eq:utilrefprice)
\end{equation}

donde:

- $P_{nit}$ es el precio actual de la alternativa $i$ para el individuo $n$ en período $t$
- $RP_{nit}$ es el **precio de referencia** (no observable) de la alternativa $i$
- $\beta_1$ captura el efecto del precio absoluto
- $\beta_2$ captura el efecto de pérdida/ganancia respecto al precio de referencia
- $x_{nit}$ son otras covariables

**Interpretación de Coeficientes:**

- **Precio absoluto ($\beta_1$):** Se espera $\beta_1 < 0$ - mayor precio reduce utilidad
- **Desviación de precio de referencia ($\beta_2$):** Se espera $\beta_2 > 0$
  - Si $P_{nit} > RP_{nit}$ (precio mayor que referencia): efecto negativo adicional - "pérdida percibida"
  - Si $P_{nit} < RP_{nit}$ (precio menor que referencia): efecto positivo adicional - "ganancia percibida"

La especificación captura que los consumidores son **más sensibles a desviaciones del precio de referencia** que al nivel de precio absoluto, consistente con teoría de prospectos que postula mayor sensibilidad a pérdidas que a ganancias.

**Construcción del Precio de Referencia:**

Similar a la lealtad, el precio de referencia se construye como promedio ponderado exponencialmente de precios pasados:

\begin{equation}
RP_{nit} = \lambda RP_{nit-1} + (1-\lambda) P_{nit-1}
(\#eq:refprice)
\end{equation}

donde $\lambda$ es el parámetro de persistencia del precio de referencia.

**Especificación asimétrica:**

La teoría de prospectos sugiere que las pérdidas (precios mayores que referencia) pesan más que las ganancias (precios menores). Esto motiva especificaciones asimétricas:

\begin{equation}
u_{nit} = \alpha_i + \beta_1 P_{nit} - \beta_2^+ \max(P_{nit} - RP_{nit}, 0) - \beta_2^- \max(RP_{nit} - P_{nit}, 0) + \delta x_{nit} + \varepsilon_{nit}
(\#eq:utilasym)
\end{equation}

donde $\beta_2^+$ captura el efecto de precios superiores a la referencia (pérdida) y $\beta_2^-$ el efecto de precios inferiores (ganancia). Típicamente se encuentra $\beta_2^+ > \beta_2^-$, confirmando aversión a pérdidas.

**Ejemplo Numérico:**

Considérese un producto con la siguiente historia de precios: \$10, \$12, \$9, \$11, ?.
Con $\lambda = 0.6$:

\begin{aligned}
RP_t &= 0.6 \cdot RP_{t-1} + 0.4 \cdot P_{t-1}\\
RP_2 &= 0.4 \cdot 10 = 4.0\\
RP_3 &= 0.6 \cdot 4.0 + 0.4 \cdot 12 = 7.2\\
RP_4 &= 0.6 \cdot 7.2 + 0.4 \cdot 9 = 7.92\\
RP_5 &= 0.6 \cdot 7.92 + 0.4 \cdot 11 = 9.152
\end{aligned}

Si en el período 5 el precio es \$13 y se tiene $\beta_1 = -0.5$ y $\beta_2 = 0.8$:

$$u_5 = \alpha + (-0.5)(13) - 0.8(13 - 9.152) + ... = \alpha - 6.5 - 3.08 + ... $$

El precio \$13 genera:

- Efecto directo: $-0.5 \times 13 = -6.5$
- Efecto de pérdida percibida: $-0.8 \times 3.848 = -3.08$
- **Efecto total:** $-9.58$ (más negativo que solo el efecto directo)

#### Combinación de Persistencia y Precios de Referencia

En la práctica, ambos fenómenos suelen coexistir. Un modelo integrado especificaría:

\begin{equation}
u_{nit} = \alpha_i + \beta_1 P_{nit} - \beta_2(P_{nit} - RP_{nit}) + \gamma z_{nit} + \delta x_{nit} + \varepsilon_{nit}
(\#eq:utilfull)
\end{equation}

donde tanto $z_{nit}$ como $RP_{nit}$ se construyen recursivamente usando los datos históricos del individuo:

\begin{aligned}
z_{nit} &= \lambda_z z_{nit-1} + (1-\lambda_z) y_{nit-1}\\
RP_{nit} &= \lambda_p RP_{nit-1} + (1-\lambda_p) P_{nit-1}
\end{aligned}

### Apéndice{-}

**Derivación probabilidad de elección modelo logit**

Por definición 

$$P_{nit} = Pr(\varepsilon_{njt}<v_{nit} - v_{njt} + \varepsilon_{nit}), \forall j \neq i$$
Fijando el valor de $\varepsilon_{nit}$, la probabilidad anterior no es más que una multiplicación de funciones de distribución de variables aleatorias valor extremo. Por lo tanto, se puede condicionar en $\varepsilon_{nit}$ y luego integrar respecto a los valores que puede tomar. Para simplificar la notación, sea $s=\varepsilon_{nit}$.


\begin{aligned}
P_{nit} &= \int_{-\infty}^{\infty} \left(\prod_{j\neq i}    e^{-e^{-(s + v_{ni} - v_{nj})}}\right) e^{-s} e^{-e^{-s}}ds\\
&= \int_{-\infty}^{\infty} \left(\prod_{j}   e^{-e^{-(s + v_{ni} - v_{nj})}}\right) e^{-s}ds\\
&= \int_{-\infty}^{\infty} exp\left(\sum_{j}  e^{-(v_{ni} - v_{nj})}\right) e^{-s}ds
\end{aligned}

Para resolver la integral, se puede recurrir a un cambio de variables $t = e^{−s}$ y $dt = e^{−s}ds$. Con esto:

\begin{aligned}
P_{nit} &= \int_{\infty}^{0} -e^{t\sum_{j}  e^{-(v_{ni} - v_{nj})}} dt\\
&= \frac{ e^{-(v_{ni} - v_{nj})}}{\sum_{j}  e^{-(v_{ni} - v_{nj})}}\mid^{\infty}_{0}\\
&= \frac{e^{v_{ni}}}{\sum_j e^{v_{nj}}}
\end{aligned}


## Probit

### Definición

Al introducir modelos de elección discreta, se postuló que los tomadores de decisiones disponían de una función de utilidad subyacente que se descomponía en una componente determinística y otra aleatoria. Más aún, se discutió que el modelo que describe la probabilidad de elegir cada una de las alternativas quedaba directamente determinado por la distribución que se asumiera para la componente aleatoria de la utilidad. Aunque una especificación de errores normales centrados en cero tiene una larga tradición en modelos econométricos, por simplicidad se optó por iniciar la discusión con modelos *logit* derivados de asumir que la componente aleatoria de la utilidad se distribuía valor extremo tipo I. En este capítulo se volverá al caso de componentes aleatorias normales que dan origen al modelo probit. Formalmente, un modelo *probit* resulta de los siguientes supuestos de comportamiento:

\begin{equation} 
  \begin{array}{cc}
  u_{ni} = v_{ni} + \varepsilon_{ni}, &\varepsilon_n \sim N(0,\Sigma)
  \end{array}  
  (\#eq:compo)
\end{equation} 

La normalidad de los errores provee bastante flexibilidad para acomodar una amplia variedad de estructuras de las preferencias. Como se verá en la discusión que sigue, un modelo con errores normales permite acomodar factores sistemáticos no observables en la utilidad. Una de las pocas limitaciones de un modelo probit viene de la normalidad de dichos factores. Por ejemplo, si se quiere incorporar el efecto que tiene el precio en la utilidad como una componente aleatoria, entonces las colas de la distribución normal implicarán una probabilidad positiva de que algunos clientes aumenten la utilidad de una alternativa si aumenta el precio de esta. Formalmente, el supuesto de la normalidad de la componente aleatoria de la utilidad implica que su función de densidad viene dada por:

\begin{equation} 
  \phi(\varepsilon_n) = \frac{1}{(2\pi)^{I/2}|\Sigma|^{1/2}}e^{-\frac{1}{2} \varepsilon'_n \Sigma^{-1}\varepsilon_n} 
  (\#eq:density)
\end{equation} 

Esta expresión no es más que la versión multivariada de la bien conocida densidad de la distribución $N(0, \sigma^2)$. La matriz $\Sigma$ corresponde a la matriz varianza-covarianza de los errores. Por tratarse de una distribución normal, la matriz $\Sigma$ es simétrica y de dimensión $I \times I$, donde $I$ es el número de alternativas disponibles para el tomador de decisión. Por ejemplo, si hay tres alternativas disponibles, la matriz $\Sigma$ tomaría la siguiente forma:

\begin{equation} 
  \Sigma = \begin{bmatrix} \sigma_{11} & \sigma_{12} & \sigma_{13}\\
 \cdot & \sigma_{22} & \sigma_{23}\\
 \cdot & \cdot & \sigma_{33}
\end{bmatrix} 
  (\#eq:matrix)
\end{equation} 

Los coeficientes en la diagonal dan cuenta de la variabilidad de la componente aleatoria de la utilidad. Así, por ejemplo, si $\sigma_{ii}$ tiene un valor alto indica que hay una fracción importante de la utilidad de la alternativa $i$ que no es capturada por el modelo de la componente sistemática. Los coeficientes fuera de la diagonal dan cuenta de la correlación de las componentes no observables de cada una de las alternativas. De este modo, si $\sigma_{ij}$ tiene un valor positivo alto indica que existe un elemento no observable importante que afecta simultáneamente las alternativas $i$ y $j$.

Como se vio en el desarrollo del modelo *logit*, una componente fundamental para estimar un modelo de elección discreta es la derivación de una expresión para la probabilidad de que cada agente elija cada alternativa en cada ocasión. Para el modelo probit, la probabilidad de que el individuo $n$ elija la alternativa $i$ viene dada por:

\begin{equation} 
  \begin{aligned}
    P_{ni} &= Pr (v_{ni} + \varepsilon_{ni} > v_{nj} + \varepsilon_{nj}), \forall j \neq i \\
    &= \int \mathbb{1}_{[v_{ni} + \varepsilon_{ni} > v_{nj} + \varepsilon_{nj}]}\phi(\varepsilon_n)d\varepsilon_n, \forall j \neq i
  \end{aligned}  
  (\#eq:modprobit)
\end{equation} 

Intuitivamente, simplemente se calcula el volumen bajo la densidad $\phi(\varepsilon_n)$ en la región en que los errores son tales que la alternativa $i$ es aquella que reporta mayor utilidad al individuo $n$. A diferencia del modelo logit, la integral sobre la densidad $\phi(\cdot)$ no tiene primitiva analítica y, por tanto, no se dispone de una fórmula cerrada para $P_{ni}$. Una aproximación estándar es usar simulación Monte Carlo. Para cada individuo $n$ y alternativa $i$, se genera un conjunto de $R$ vectores de errores $\{\varepsilon_n^{(r)}\}_{r=1}^R$ desde la distribución normal multivariada especificada. La probabilidad de elección se aproxima mediante:

\begin{equation}
\hat{P}_{ni}(\theta) = \frac{1}{R}\sum_{r=1}^R \mathbb{1}\left[v_{ni}(x_{ni},\theta) + \varepsilon_{ni}^{(r)} > v_{nj}(x_{nj},\theta) + \varepsilon_{nj}^{(r)}, \forall j \neq i\right]
(\#eq:simprob)
\end{equation}

donde $\mathbb{1}\{\cdot\}$ es la función indicadora. La log-verosimilitud simulada se construye como $\sum_n\sum_i y_{ni}\log\hat{P}_{ni}(\theta)$ y se maximiza numéricamente con respecto a $\theta$. A medida que $R \to \infty$, el estimador de máxima verosimilitud simulada converge al estimador de máxima verosimilitud verdadero.

### Patrones de substitución

Una de las grandes ventajas de un modelo *probit* es su flexibilidad para capturar una amplia variedad de patrones de comportamiento. En efecto, un modelo *probit* no impone restricciones en los patrones de sustitución más allá de la simetría propia de la distribución normal, lo que posibilita al analista explorar el esquema que mejor se ajusta a los datos. En este sentido, es útil compararlo con el modelo *logit* que, aunque provee una fórmula analítica cerrada para la probabilidad de cada elección, impone la propiedad de sustitución proporcional (o de independencia de alternativas irrelevantes). El modelo probit no tiene esta propiedad y, por tanto, el aumento de la probabilidad de elección de una alternativa puede tener impactos diferentes en las probabilidades de elección de las alternativas remanentes. Esto permitiría, por ejemplo, identificar pares de alternativas que son mejores sustitutos (complementos) más allá de las comunalidades que podrían existir en las componentes determinísticas de su utilidad. 

A continuación se discutirá cómo el modelo probit puede ser usado para representar algunas situaciones de elección discreta.

#### Variación aleatoria en preferencias

Una de las componentes más importantes en el diseño de un plan comercial exitoso es la identificación de cómo las preferencias de los potenciales clientes se distribuyen en la población. Identificando estas variaciones, se pueden encontrar las propuestas de valor que resulten más atractivas para cada grupo de clientes. En un modelo probit, se puede asumir que los parámetros que definen la componente determinística son heterogéneos en la población sin perder los supuestos básicos que definen el modelo. Por simplicidad, supóngase que la componente determinística de la utilidad es lineal:

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = \beta'_{n} x_{ni} + \varepsilon_{ni}, & \varepsilon_n \sim N(0,\Sigma)
  \end{array}  
  (\#eq:cdeterminista)
\end{equation} 

Nótese que, a diferencia de los modelos anteriores, ahora se ha asumido que cada tomador de decisión $n$ tiene su propio conjunto de parámetros $\beta_n$ que describen sus preferencias por las alternativas disponibles. Para completar el modelo se necesita especificar una distribución de $\beta_n$ en la población. Para mantener la estructura del modelo se asumirá normalidad: $\beta_n \sim N(b, \sigma^2_\beta)$. Dado que la suma de dos variables aleatorias normales se distribuye normal, es fácil ver que el modelo es equivalente a:

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = b'_{n} x_{ni} + \eta_{ni}, & \eta_n \sim (0,\hat{\Sigma})
  \end{array}  
  (\#eq:cdeterministaequi)
\end{equation} 

Las componentes de la matriz de varianza-covarianza resultante $\hat{\Sigma}$ pueden trazarse directamente a las componentes de la matriz $\Sigma$ original como lo indica el siguiente ejemplo:

**Ejemplo:** Considérese un modelo de elección con dos alternativas y un modelo lineal con una única variable para describir la componente sistemática de la utilidad. En este caso, las utilidades por cada alternativa vienen dadas por:

\begin{aligned}
u_{n1} &= \beta_n x_{n1} + \varepsilon_{n1}\\
u_{n2} &= \beta_n x_{n2} + \varepsilon_{n2}
\end{aligned}

donde $\varepsilon_{n1}$ y $\varepsilon_{n2}$ son términos independientes e idénticamente distribuidos con varianza $\sigma_\varepsilon$. Si se asume que el parámetro $\beta_n$ se distribuye normal con media $b$ y varianza $\sigma_\beta$, entonces se puede reescribir las utilidades como:

\begin{aligned}
u_{n1} &= b x_{n1} + \eta_{n1}\\
u_{n2} &= b x_{n2} + \eta_{n2}
\end{aligned}

donde $\eta_{n1}$ y $\eta_{n2}$ están normalmente distribuidas. Cada una tiene esperanza cero: $\mathbb{E}(\eta_{ni}) =\mathbb{E}(\beta_nx_{ni} + \varepsilon_{ni}) = 0$, varianza igual a $\mathbb{V}ar(\eta_{ni}) = \mathbb{V}ar(\beta_n x_{ni} + \varepsilon_{ni}) = x^2_{ni}\sigma_\beta + \sigma_\varepsilon$ y covarianzas $\mathbb{C}ov(\eta_{n1}, \eta_{n2}) = x_{n1}x_{n2}\sigma_\beta$. Así, la matriz de covarianza viene dada por:

\begin{aligned}
\Sigma &= \begin{bmatrix}x^2_{n1}\sigma_\beta + \sigma_\varepsilon & x_{n1}x_{n2}\sigma_\beta\\
x_{n1}x_{n2}\sigma_\beta & x^2_{n2}\sigma_\beta + \sigma_\varepsilon \end{bmatrix} \\
&= \sigma_\beta \begin{bmatrix}x^2_{n1} & x_{n1}x_{n2}\\
x_{n1}x_{n2} & x^2_{n2} \end{bmatrix} + \sigma_{\varepsilon} \begin{bmatrix}1 & 0\\
0 & 1\end{bmatrix}
\end{aligned}

El siguiente paso es estimar. Recordando que el comportamiento no es afectado por transformaciones multiplicativas de la utilidad, es necesario escalar esta matriz. Lo recomendable es fijar $\sigma_\varepsilon = 1$, obteniendo así:

$$Σ= \sigma_\beta \begin{bmatrix}x^2_{n1} & x_{n1}x_{n2}\\
x_{n1}x_{n2} & x^2_{n2} \end{bmatrix} + \begin{bmatrix}1 & 0\\
0 & 1\end{bmatrix}$$


#### Dependencia del tiempo

Se ha discutido que bajo un modelo *probit* se pueden estudiar relaciones no observables entre las alternativas de elección. En las bases disponibles para la función comercial, las observaciones suelen estar indexadas temporalmente generando estructuras de panel que permiten estudiar aspectos interesantes de los agentes. A continuación se discute cómo usar un modelo *probit* para explorar no solo la relación entre las utilidades de alternativas, sino que también, el comportamiento de las utilidades de las alternativas en el tiempo. 

Al igual que en la sección anterior, se busca encontrar patrones temporales en las componentes no observables de la utilidad, ya que las variaciones en la componente observable pueden ser fácilmente estudiadas incluyendo variables observables que describan la evolución temporal del sistema. Por ejemplo, si se cree que la utilidad de una de las alternativas es creciente en el tiempo, basta incluir el tiempo $t$ entre las variables independientes en la descripción de la utilidad de la alternativa. En general, se debería esperar que las utilidades estén correlacionadas tanto en el tiempo como entre las alternativas, ya que los factores que no son observados por el analista suelen ser persistentes en el tiempo. Eventualmente un modelo *probit* también podría ayudar a identificar shocks en que hay variaciones instantáneas (o de unos pocos períodos) en las utilidades de varias de las alternativas. 

Supóngase que se observa un panel de $N$ clientes que deciden respecto de $I$ alternativas en $T$ períodos y que la utilidad del producto que el agente $n$ deriva sobre la alternativa $i$ en el período $t$ viene dada por:

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = v_{nit} + \varepsilon_{nit},  & [\varepsilon_{n11},...,\varepsilon_{nI1},\varepsilon_{n12}, ..., \varepsilon_{nI2},...,\varepsilon_{n1T},...,\varepsilon_{nIT}] \sim N(0,Σ)
  \end{array}  
  (\#eq:cdeterministaequi)
\end{equation} 

La matriz de covarianza \Sigma tiene dimensión $IT \times IT$ (como se verá, no todas las componentes son identificables y se deberá imponer ciertas restricciones). Para paneles típicos, $T$ es grande y genera matrices de varianza-covarianza muy grandes. Por ejemplo, si se tienen datos semanales de compras de 5 marcas por un período de 2 años, se enfrentará una matriz de varianza (sin normalizar) con 5 × 104 = 520 filas y 520 columnas, lo que generaría no solo un modelo difícil de estimar numéricamente sino que, también, difícil de interpretar. Así, para usar un modelo *probit* con dependencia en el tiempo, típicamente se agregará estructura al modelo. Por ejemplo, se puede restringir el análisis a grupos de períodos que podría ser el caso de las decisiones antes y después de una intervención en el sistema (e.g., antes y después del lanzamiento de una campaña publicitaria).

**Ejemplo:** Supóngase un caso de elección binaria. El error está compuesto por una componente sistemática específica del tomador de decisión y otra que es variable en el tiempo.


\begin{equation} 
  \varepsilon_{nt} = \eta_n + \mu_{nt}
  (\#eq:errorbinario)
\end{equation} 

Si se asume que $η_n$ está distribuida $N(0, σ)$ y $\mu_{nt}$ en $N(0, 1)$, entonces la varianza y covarianza son

  \begin{equation} 
  \mathbb{V}ar(\varepsilon_{nt}) =  \mathbb{V}ar(\eta_{n}+ \mu_{nt}) = \sigma + 1
    (\#eq:varbinaria)
  \end{equation} 
  
  \begin{equation} 
  \mathbb{C}ov(\varepsilon_{nt},\varepsilon_{ns}) =  \mathbb{E}((\eta_{n}+ \mu_{nt}) (\eta_{n}+ \mu_{ns})) = \sigma
    (\#eq:covbinaria)
  \end{equation} 

La matriz $\Sigma$, por lo tanto, es

\begin{equation} 
  Σ = \begin{bmatrix}\sigma +1 & \sigma & ... & \sigma\\
\sigma & \sigma+1 & ... & \sigma\\
\vdots & \vdots & \ddots & \vdots\\
\sigma & \sigma & \dots & \sigma +1
\end{bmatrix}
    (\#eq:matrizsum)
  \end{equation} 

### Identificación

Para estimar un modelo *probit*, junto con los parámetros de la componente sistemática de la utilidad, se necesita estimar los coeficientes de la matriz $\Sigma$. Por tratarse de una distribución normal, la matriz $\Sigma$ es simétrica, ya que sus varianzas generadas son conmutativas. Por tanto, se deben estimar $\frac{I(I+1)}{2}$ de sus componentes. Sin embargo, dicho problema no es identificable y se necesita imponer restricciones adicionales eliminando uan componente. La intuición detrás de esta falta de identificación resulta de asumir que las utilidades subyacentes que maximizan los individuos son monótonas y homotéticas. En otras palabras, se puede agregar un valor constante a las utilidades de cada una de las alternativas o escalarlas en cualquier proporción y la identidad de la alternativa de mayor utilidad no cambia. En general, si se tienen $I$ alternativas, solo se pueden identificar $\frac{I(I+1)}{2} - 1$ parámetros. A continuación se discutirán dos enfoques para generar restricciones que hagan el problema identificable.

#### Normalización de las funciones de utilidad

Motivados en las propiedades de la función de utilidad, este enfoque consiste en imponer directamente restricciones de escala y locación. Este enfoque es completamente general y permite además garantizar identificación con un procedimiento estándar que puede incluso automatizarse. Formalmente el proceso consiste en imponer dos restricciones:

1. FIJAR LOCACIÓN: Como el valor absoluto de las utilidades es irrelevante, se puede fijar arbitrariamente el punto de referencia sobre el cual se interpretarán las utilidades. De esta forma, se tomará la utilidad de una de las alternativas como referencia y se redefinirán las utilidades como las diferencias con respecto a la alternativa de referencia.

2. FIJAR ESCALA: Como la escala de las utilidades es irrelevante, se puede fijarla asignando un valor arbitrario a cualquiera de las componentes de la matriz de varianza covarianza. Típicamente se impondrá que la primera componente de la diagonal tome el valor 1.
 
**Ejemplo:** Considérese la normalización de una matriz $\Sigma$ resultante de un problema de elección discreto de 4 alternativas. 

\begin{equation} 
  \Sigma = \begin{bmatrix} \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{13}\\
 \cdot & \sigma_{22} & \sigma_{23} & \sigma_{24}\\
 \cdot & \cdot & \sigma_{33} & \sigma_{34}\\
 \cdot & \cdot & \cdot & \sigma_{44}
\end{bmatrix} 
  (\#eq:matrixcuatro)
\end{equation} 


El primer paso en la normalización es considerar diferencias de utilidades con respecto a una alternativa de referencia, la que por simplicidad se escogerá como la primera de la lista. Al fijar esta utilidad y tomar las diferencias, se ha reducido la dimensión del vector de errores, resultando en una matriz de varianza-covarianza $\hat{Σ} = \{\hat{σ}_{ij}\}^3_{i,j=1}$ cuyas componentes vienen dadas por:

\begin{aligned}
\hat{\sigma}_{22} &= \sigma_{22} + \sigma_{11} - 2\sigma_{12}\\
\hat{\sigma}_{33} &= \sigma_{33} + \sigma_{11} - 2\sigma_{13}\\
\hat{\sigma}_{44} &= \sigma_{44} + \sigma_{11} - 2\sigma_{14}\\
\hat{\sigma}_{23} &= \sigma_{23} + \sigma_{11} - \sigma_{12} - \sigma_{13}\\
\hat{\sigma}_{24} &= \sigma_{24} + \sigma_{11} - \sigma_{12} - \sigma_{14}\\
\hat{\sigma}_{34} &= \sigma_{34} + \sigma_{11} - \sigma_{13} - \sigma_{14}
\end{aligned}

El segundo paso en la normalización es fijar en 1 (o cualquier otro real positivo) una de las componentes de la diagonal de la matriz de varianza covarianza para precisar la escala de la función de utilidad. Por simplicidad se escoge la primera componente de la diagonal. Para hacerla 1 basta con dividir toda la matriz por dicha componente, resultando en una matriz de varianza-covarianza  $\hat{Σ} = \{\hat{σ}_{ij}\}^3_{i,j=1}$ cuyas componentes vienen dadas por:

\begin{aligned}
\hat{\sigma}_{33} &= \frac{\sigma_{33} + \sigma_{11} - 2\sigma_{13}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{44} &= \frac{\sigma_{44} + \sigma_{11} - 2\sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{23} &= \frac{\sigma_{23} + \sigma_{11} - \sigma_{12} - \sigma_{13}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{24} &= \frac{\sigma_{24} + \sigma_{11} - \sigma_{12} - \sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{34} &= \frac{\sigma_{34} + \sigma_{11} - \sigma_{13} - \sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}
\end{aligned}

La matriz resultante $\hat{Σ}$ es identificable. En ella es importante trazar sus componentes originales de la matriz sigma porque ayudan a darle interpretación a los resultados obtenidos en la estimación.

#### Incorporación de restricciones estructurales

Aunque completamente general, la normalización descrita en la sección anterior, muchas veces puede ser algo inconveniente en cuanto los parámetros estimados no tienen interpretación
directa. Un enfoque que permite interpretar directamente los parámetros se obtiene al imponer estructura sobre la matriz de varianza-covarianza a partir de supuestos de comportamiento. Por ejemplo, se puede imponer que las componentes aleatorias de algunos pares de alternativas no están correlacionadas o que algún grupo de alternativas tiene la misma variabilidad de la componente no observable. El cuadro 4.1 ejemplifica algunas de las estructuras de varianza-covarianza comúnmente usadas en la literatura.

Otros modelos usados en la literatura y que están implementados en aplicaciones comerciales incluyen estructuras de bandas, Huynh-Feldt, autoregresivo heterogéneo y simetría compuesta. Como en otros aspectos de la modelación, la elección de la estructura a elegir para la matriz de
varianza-covarianza dependerá de las hipótesis de comportamiento que se tengan a la mano y la dificultad numérica de estimar el modelo resultante. 


## Nested Logit

### Introducción

Como se ha discutido previamente, una de las limitaciones fundamentales del modelo logit estándar es la propiedad de independencia de alternativas irrelevantes (IIA), que implica patrones de sustitución proporcionales entre todas las alternativas. Esta restricción puede resultar poco realista en muchas situaciones prácticas de marketing donde algunas alternativas son sustitutos más cercanos entre sí que con otras. El modelo **Nested Logit** (o Logit Anidado) surge como una extensión del modelo logit estándar que permite relajar parcialmente la restricción de IIA al permitir que las alternativas se agrupen en conjuntos (o "nidos") donde la correlación entre alternativas dentro del mismo nido puede diferir de la correlación entre alternativas de nidos diferentes.

La idea fundamental del nested logit es reconocer que en muchos problemas de elección existe una estructura jerárquica natural. Los tomadores de decisión primero eligen entre categorías generales de alternativas (los nidos) y luego, dentro de la categoría seleccionada, eligen una alternativa específica. Esta estructura refleja de manera más realista muchos procesos de decisión en marketing.

Por ejemplo, se tiene el problema de elección de medio de transporte en una ciudad que ofrece las siguientes alternativas: auto particular, auto compartido (carpool), bus y tren. Intuitivamente, se esperaría que auto particular y auto compartido sean sustitutos más cercanos entre sí (ambos involucran viajar en auto) que cualquiera de ellos con respecto al bus o tren. Del mismo modo, bus y tren (ambos transporte público) serían sustitutos más cercanos entre sí. Un modelo logit estándar impondría que un incremento en el costo del bus afecta igualmente la probabilidad de elegir auto particular, auto compartido y tren, lo cual parece poco razonable. El nested logit permite capturar que el incremento en el costo del bus afectará más la probabilidad de elegir tren (mismo nido de transporte público) que la de elegir auto particular.

### Estructura del Modelo

#### Partición en Nidos

El modelo nested logit requiere una partición del conjunto de $J$ alternativas en $K$ subconjuntos exhaustivos y mutuamente excluyentes llamados **nidos**. Sea $B_k$ el conjunto de alternativas en el nido $k$, donde $k = 1, 2, ..., K$. La partición debe satisfacer:

1. $\bigcup_{k=1}^K B_k = \{1, 2, ..., J\}$ (exhaustividad)
2. $B_k \cap B_m = \emptyset$ para $k \neq m$ (exclusividad mutua)

Es importante destacar que la definición de los nidos debe basarse en consideraciones teóricas sobre qué alternativas son sustitutos más cercanos, no en criterios puramente estadísticos. La teoría económica, el conocimiento del mercado y la intuición sobre el comportamiento del consumidor deben guiar esta partición.

#### Especificación de la Utilidad

En el modelo nested logit, la utilidad que el individuo $n$ deriva de la alternativa $i$ en el nido $B_k$ se especifica como:

\begin{equation}
u_{ni} = v_{ni} + \varepsilon_{ni}
(\#eq:nestedutil)
\end{equation}

donde $v_{ni}$ es la componente sistemática (observable) de la utilidad y $\varepsilon_{ni}$ es la componente aleatoria. Si bien es común en la literatura descomponer el error $\varepsilon_{ni}$ en dos componentes —una idiosincrática a la alternativa y otra común a todas las alternativas del nido— esta descomposición no es necesaria para derivar el modelo. Como muestra Train (2009), la estructura de correlación del nested logit puede motivarse directamente asumiendo que el vector completo de errores sigue una distribución de valor extremo generalizada (GEV), sin necesidad de imponer una estructura aditiva específica en los términos de error. Esta formulación más general permite capturar la correlación intra-nido a través de los parámetros de la distribución GEV.

La característica distintiva del nested logit es la estructura de correlación de los errores. A diferencia del logit estándar donde todos los $\varepsilon_{ni}$ son independientes, en el nested logit se permite que los errores de alternativas dentro del mismo nido estén correlacionados, mientras que los errores entre alternativas de diferentes nidos permanecen independientes.

Formalmente, se asume que el vector de errores $\varepsilon_n = (\varepsilon_{n1}, ..., \varepsilon_{nJ})$ tiene una distribución de valor extremo con función de distribución acumulada:

\begin{equation}
F(\varepsilon_{n1}, ..., \varepsilon_{nJ}) = \exp\left[-\sum_{k=1}^K \left(\sum_{i \in B_k} e^{-\varepsilon_{ni}/\lambda_k}\right)^{\lambda_k}\right]
(\#eq:gevdist)
\end{equation}

donde $\lambda_k$ es un parámetro que gobierna el grado de correlación entre alternativas en el nido $k$, con $0 < \lambda_k \leq 1$:

- **$\lambda_k = 1$:** Las alternativas en el nido $k$ no están correlacionadas (caso del logit estándar)
- **$\lambda_k < 1$:** Las alternativas en el nido $k$ están positivamente correlacionadas; cuanto menor sea $\lambda_k$, mayor es la correlación
- **$\lambda_k \to 0$:** Correlación perfecta entre alternativas del nido $k$

La correlación entre dos alternativas $i$ y $j$ en el mismo nido $k$ viene dada por:

\begin{equation}
\text{Corr}(\varepsilon_{ni}, \varepsilon_{nj}) = 1 - \lambda_k^2, \quad \text{para } i, j \in B_k
(\#eq:corrwithin)
\end{equation}

Mientras que alternativas en nidos diferentes tienen correlación cero:

\begin{equation}
\text{Corr}(\varepsilon_{ni}, \varepsilon_{nj}) = 0, \quad \text{para } i \in B_k, j \in B_m, k \neq m
(\#eq:corrbetween)
\end{equation}

### Probabilidades de Elección

Una de las ventajas computacionales del modelo nested logit es que, a pesar de permitir correlación entre errores, aún se pueden derivar fórmulas cerradas para las probabilidades de elección.

#### Estructura Jerárquica de Decisión

El nested logit puede interpretarse como un proceso de decisión secuencial en dos etapas:

1. **Etapa 1 (elección de nido):** El individuo elige qué nido $B_k$ le proporciona mayor utilidad esperada
2. **Etapa 2 (elección dentro del nido):** Dado el nido seleccionado, el individuo elige la alternativa específica dentro de ese nido

#### Probabilidad Condicional (Etapa 2)

La probabilidad de que el individuo $n$ elija la alternativa $i$ dado que ha elegido el nido $B_k$ viene dada por:

\begin{equation}
P_{ni|B_k} = \frac{\exp(v_{ni}/\lambda_k)}{\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)}
(\#eq:condprob)
\end{equation}

Esta expresión tiene la forma de un modelo logit estándar, escalando las utilidades con $\lambda_k$.

#### Valor Inclusivo (Inclusive Value)

Un concepto fundamental en el nested logit es el **valor inclusivo** (o **log-suma**) del nido $B_k$, definido como:

\begin{equation}
IV_k = \ln\left(\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right)
(\#eq:inclusivevalue)
\end{equation}

El valor inclusivo $IV_k$ representa la utilidad esperada (antes de conocer los shocks aleatorios $\varepsilon_{nj}$) que el individuo obtiene del nido $k$. Es una medida del atractivo agregado de todas las alternativas en el nido, ajustada por el grado de correlación entre ellas.

**Interpretación Económica:** El valor inclusivo captura el "valor de la opción" que proporciona tener múltiples alternativas similares disponibles. Un valor inclusivo alto indica que el nido contiene alternativas atractivas para el individuo.

#### Probabilidad Marginal de Elegir el Nido (Etapa 1)

La probabilidad de elegir el nido $B_k$ viene dada por:

\begin{equation}
P_{nB_k} = \frac{\exp(\lambda_k \cdot IV_k)}{\sum_{m=1}^K \exp(\lambda_m \cdot IV_m)}
(\#eq:nestprob)
\end{equation}

El valor $m$ corresponde al índice que suma las utilidades de todos las opciones del nido $k$. Esta probabilidad tiene también forma logit, donde la "utilidad" del nido es el valor inclusivo escalado por $\lambda_k$.

#### Probabilidad No Condicional (Probabilidad Total)

Finalmente, la probabilidad de que el individuo $n$ elija la alternativa $i$ en el nido $B_k$ se obtiene multiplicando las probabilidades de las dos etapas:

\begin{equation}
P_{ni} = P_{ni|B_k} \cdot P_{nB_k} = \frac{\exp(v_{ni}/\lambda_k)}{\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)} \cdot \frac{\exp(\lambda_k \cdot IV_k)}{\sum_{m=1}^K \exp(\lambda_m \cdot IV_m)}
(\#eq:totalprob)
\end{equation}

Para reescribir esta expresión de forma más compacta, se procede de la siguiente manera. Recordando que $IV_k = \ln\left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]$, se tiene que:

$$\exp(\lambda_k \cdot IV_k) = \exp\left(\lambda_k \cdot \ln\left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]\right) = \left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]^{\lambda_k}$$

Sustituyendo esta expresión en el numerador de la probabilidad total:

\begin{aligned}
P_{ni} &= \frac{\exp(v_{ni}/\lambda_k)}{\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)} \cdot \frac{\left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]^{\lambda_k}}{\sum_{m=1}^K \left[\sum_{j \in B_m} \exp(v_{nj}/\lambda_m)\right]^{\lambda_m}}\\
&= \frac{\exp(v_{ni}/\lambda_k) \cdot \left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]^{\lambda_k}}{\left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right] \cdot \sum_{m=1}^K \left[\sum_{j \in B_m} \exp(v_{nj}/\lambda_m)\right]^{\lambda_m}}\\
&= \frac{\exp(v_{ni}/\lambda_k) \cdot \left[\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)\right]^{\lambda_k - 1}}{\sum_{m=1}^K \left[\sum_{j \in B_m} \exp(v_{nj}/\lambda_m)\right]^{\lambda_m}}
\end{aligned}

Esto conduce a la forma compacta:

\begin{equation}
P_{ni} = \frac{\exp(v_{ni}/\lambda_k) \cdot [\sum_{j \in B_k} \exp(v_{nj}/\lambda_k)]^{\lambda_k - 1}}{\sum_{m=1}^K [\sum_{j \in B_m} \exp(v_{nj}/\lambda_m)]^{\lambda_m}}
(\#eq:totalprobcompact)
\end{equation}

### Propiedades y Patrones de Sustitución

#### Relajación Parcial de IIA

Una propiedad importante del nested logit es que la restricción de IIA se mantiene **dentro de cada nido** pero no **entre nidos**. Específicamente:

**Dentro del mismo nido:** Para dos alternativas $i$ y $j$ en el mismo nido $B_k$:

\begin{equation}
\frac{P_{ni}}{P_{nj}} = \frac{\exp(v_{ni}/\lambda_k)}{\exp(v_{nj}/\lambda_k)} = \exp\left(\frac{v_{ni} - v_{nj}}{\lambda_k}\right)
(\#eq:ratiowithin)
\end{equation}

Este ratio es independiente de otras alternativas, manteniendo IIA dentro del nido.

**Entre diferentes nidos:** Para alternativas en nidos diferentes, el ratio de probabilidades sí depende de las características de otras alternativas a través de los valores inclusivos, permitiendo patrones de sustitución más realistas.

#### Elasticidades de Sustitución

Las elasticidades propias y cruzadas en un modelo nested logit revelan cómo los patrones de sustitución difieren entre alternativas del mismo nido versus alternativas de nidos diferentes.

**Elasticidad propia:** La elasticidad de $P_{ni}$ respecto a un atributo $x_{ni}$ de la misma alternativa es:

\begin{equation}
\eta_{ii} = \frac{\partial P_{ni}}{\partial x_{ni}} \cdot \frac{x_{ni}}{P_{ni}} = \frac{\beta}{\lambda_k} x_{ni} \left(1 - P_{ni} - \lambda_k P_{ni|B_k}(1 - P_{ni|B_k})\right)
(\#eq:elastown)
\end{equation}

donde $\beta$ es el coeficiente asociado a $x_{ni}$ en $v_{ni}$.

**Elasticidad cruzada dentro del nido:** Para alternativas $i$ y $j$ en el mismo nido $B_k$:

\begin{equation}
\eta_{ij} = \frac{\partial P_{ni}}{\partial x_{nj}} \cdot \frac{x_{nj}}{P_{ni}} = \frac{\beta}{\lambda_k} x_{nj} P_{nj|B_k}(1 + \lambda_k(1 - P_{nj|B_k}))
(\#eq:elastwithin)
\end{equation}

**Elasticidad cruzada entre nidos:** Para alternativas $i \in B_k$ y $j \in B_m$ con $k \neq m$:

\begin{equation}
\eta_{ij} = -\beta x_{nj} P_{nj}
(\#eq:elastbetween)
\end{equation}

Nótese que $\eta_{ij}$ (dentro del nido) depende de $\lambda_k$ y es típicamente mayor en magnitud que la elasticidad cruzada entre nidos, reflejando que alternativas dentro del mismo nido son mejores sustitutos.

### Ejemplo

En una situación de elección de tipo de vivienda en una ciudad hay cuatro alternativas: departamento pequeño (DP), departamento grande (DG), casa pequeña (CP) y casa grande (CG).

**Estructura de Nidos:** Se propone una estructura con dos nidos:

- Nido 1 (Departamentos): $B_1 = \{DP, DG\}$
- Nido 2 (Casas): $B_2 = \{CP, CG\}$

**Especificación de Utilidad:**

\begin{aligned}
v_{n,DP} &= \alpha_{DP} + \beta_1 \cdot precio_{DP} + \beta_2 \cdot distancia_{DP}\\
v_{n,DG} &= \alpha_{DG} + \beta_1 \cdot precio_{DG} + \beta_2 \cdot distancia_{DG}\\
v_{n,CP} &= \alpha_{CP} + \beta_1 \cdot precio_{CP} + \beta_2 \cdot distancia_{CP}\\
v_{n,CG} &= \alpha_{CG} + \beta_1 \cdot precio_{CG} + \beta_2 \cdot distancia_{CG}
\end{aligned}

donde $\alpha_i$ son constantes alternativa-específicas, $\beta_1 < 0$ captura el efecto del precio y $\beta_2 < 0$ el efecto de la distancia al centro.

**Resultados Hipotéticos:**

Supóngase que la estimación arroja $\lambda_1 = 0.6$ y $\lambda_2 = 0.7$. Esto implica:

- **Correlación en Nido 1 (Departamentos):** $1 - (0.6)^2 = 0.64$
- **Correlación en Nido 2 (Casas):** $1 - (0.7)^2 = 0.51$

Los departamentos presentan mayor correlación en sus componentes no observables que las casas, sugiriendo que hay factores no modelados (quizás preferencias por estilo de vida urbano, servicios comunes en edificios, etc.) que afectan similarmente la atractiva de ambos tipos de departamentos.

**Implicaciones para Sustitución:**

Si el precio del departamento grande aumenta, este incremento afectará más la probabilidad de elegir el departamento pequeño (mismo nido) que la probabilidad de elegir cualquiera de las casas. Formalmente, la elasticidad cruzada $\eta_{DP,DG}$ será mayor en magnitud que $\eta_{DP,CP}$ o $\eta_{DP,CG}$.

### Estimación

La estimación del modelo nested logit se realiza mediante **máxima verosimilitud**. Dada una muestra de $N$ individuos donde cada individuo $n$ elige una alternativa $i_n$ de su conjunto de elección, la función de log-verosimilitud viene dada por:

\begin{equation}
LL(\theta, \lambda) = \sum_{n=1}^N \ln P_{ni_n}(\theta, \lambda)
(\#eq:nestedll)
\end{equation}

donde $\theta$ representa todos los parámetros de las utilidades sistemáticas y $\lambda = (\lambda_1, ..., \lambda_K)$ son los parámetros de disimilitud de cada nido.

**Procedimiento de Estimación:**

1. Especificar la partición en nidos $\{B_1, ..., B_K\}$
2. Especificar las funciones de utilidad $v_{ni}$ para cada alternativa
3. Calcular valores inclusivos $IV_k$ para cada nido
4. Calcular probabilidades $P_{ni}$ usando la ecuación \@ref(eq:totalprob)
5. Maximizar la log-verosimilitud \@ref(eq:nestedll) con restricciones $0 < \lambda_k \leq 1$

**Test de Especificación:**

Un test natural es evaluar si $\lambda_k = 1$ para todos los nidos, lo que equivaldría al modelo logit estándar. Se puede usar un **test de razón de verosimilitud** comparando:

- Modelo restringido: Logit estándar (todos los $\lambda_k = 1$)
- Modelo no restringido: Nested logit con $\lambda_k$ estimados

El estadístico $LR = 2(LL_{nested} - LL_{logit})$ se distribuye asintóticamente como $\chi^2$ con $K$ grados de libertad bajo la hipótesis nula de que el logit estándar es adecuado.

### Extensiones: Nested Logit con Múltiples Niveles

El modelo nested logit puede extenderse a **estructuras jerárquicas de múltiples niveles**, donde los nidos pueden a su vez contener sub-nidos. Por ejemplo, en el contexto de elección de vehículos, se podría tener:

- **Nivel 1:** Tipo de combustión (gasolina vs. eléctrico)
- **Nivel 2:** Tamaño del vehículo (pequeño, mediano, grande)
- **Nivel 3:** Marca específica

En este caso, la decisión se modela como una secuencia de elecciones desde el nivel más alto (combustión) hasta el más específico (marca), con parámetros de disimilitud en cada nivel capturando la correlación entre alternativas dentro de cada sub-nido.

La probabilidad de elección en un nested logit de tres niveles sigue una estructura análoga:

\begin{equation}
P_{ni} = P_{ni|nido_3} \cdot P_{nido_3|nido_2} \cdot P_{nido_2}
(\#eq:threelevel)
\end{equation}

donde cada probabilidad condicional tiene forma logit con su respectivo parámetro de disimilitud.

## Mixed Logit

### Introducción y Motivación

Como se ha discutido anteriormente, el modelo logit estándar, aunque computacionalmente conveniente gracias a su forma cerrada, presenta limitaciones importantes que pueden restringir su aplicabilidad en contextos reales de marketing. El modelo **Mixed Logit** (también conocido como **Random Parameters Logit** o **Error Components Logit**) surgió como una generalización flexible del modelo logit estándar que permite superar las limitaciones de *Logit*. La idea fundamental es permitir que los parámetros de la función de utilidad varíen aleatoriamente en la población, capturando así heterogeneidad no observable en las preferencias.

**Intuición del Modelo:**

Supóngase que se está modelando la elección de medio de transporte (auto, bus, tren) y se sabe que la sensibilidad al tiempo de viaje varía sustancialmente entre individuos. Mientras que el logit estándar asumiría un único coeficiente de tiempo de viaje para toda la población, el mixed logit permite que este coeficiente siga una distribución en la población (por ejemplo, normal con cierta media y varianza), reflejando que algunos individuos son muy sensibles al tiempo mientras otros lo son menos.

### Especificación del Modelo

#### Función de Utilidad y Probabilidad de Elección

En un modelo mixed logit, la utilidad que el individuo $n$ deriva de la alternativa $i$ en la ocasión de elección $t$ viene dada por:

\begin{equation}
u_{nit} = \beta'_n x_{nit} + \varepsilon_{nit}
(\#eq:mixedutil)
\end{equation}

donde:

- $x_{nit}$ es un vector de atributos observables de la alternativa $i$ para el individuo $n$ en el período $t$
- $\beta_n$ es un vector de coeficientes **aleatorios** que varía entre individuos
- $\varepsilon_{nit}$ es un término de error i.i.d. valor extremo tipo I (como en el logit estándar)

La característica clave es que $\beta_n$ no es un parámetro fijo, sino una **variable aleatoria** que sigue una distribución de probabilidad $f(\beta | \theta)$, donde $\theta$ son los parámetros que describen esta distribución (típicamente media y varianza/covarianza).

**Especificación común:** Se suele asumir que $\beta_n \sim N(b, \Sigma_\beta)$, donde $b$ es el vector de medias poblacionales y $\Sigma_\beta$ es la matriz de varianzas-covarianzas.

Condicional en $\beta_n$, la probabilidad de que el individuo $n$ elija la alternativa $i$ es simplemente un logit estándar:

\begin{equation}
L_{nit}(\beta_n) = \frac{\exp(\beta'_n x_{nit})}{\sum_{j=1}^J \exp(\beta'_n x_{njt})}
(\#eq:condlogit)
\end{equation}

Sin embargo, como $\beta_n$ no es observable, la probabilidad **no condicional** de elección se obtiene integrando sobre la distribución de $\beta$:

\begin{equation}
P_{nit} = \int L_{nit}(\beta) f(\beta | \theta) d\beta = \int \frac{\exp(\beta' x_{nit})}{\sum_{j=1}^J \exp(\beta' x_{njt})} f(\beta | \theta) d\beta
(\#eq:mixedprob)
\end{equation}

Esta integral **no tiene solución cerrada** en general, lo que distingue fundamentalmente al mixed logit del logit estándar y requiere métodos de simulación para su estimación. La idea básica es aproximar la integral usando simulación de Monte Carlo:

1. **Extraer valores aleatorios:** Para cada individuo $n$, se extraen $R$ valores de $\beta$ de la distribución especificada $f(\beta|\theta)$: $\beta^{(1)}, \beta^{(2)}, ..., \beta^{(R)}$

2. **Calcular logit condicional:** Para cada extracción $r$, se calcula la probabilidad logit condicional:
   $$L_{nit}(\beta^{(r)}) = \frac{\exp(\beta^{(r)'} x_{nit})}{\sum_j \exp(\beta^{(r)'} x_{njt})}$$

3. **Promediar:** La probabilidad simulada es el promedio sobre las $R$ extracciones:
   \begin{equation}
   \tilde{P}_{nit} = \frac{1}{R} \sum_{r=1}^R L_{nit}(\beta^{(r)})
   (\#eq:simprob)
   \end{equation}

La probabilidad simulada $\tilde{P}_{nit}$ es un **estimador insesgado** de la verdadera probabilidad $P_{nit}$ y por la ley de los grandes números, $\tilde{P}_{nit} \to P_{nit}$ cuando $R \to \infty$.

La log-verosimilitud simulada viene dada por:

\begin{equation}
SLL(\theta) = \sum_{n=1}^N \sum_{t=1}^T \sum_{i=1}^J y_{nit} \ln(\tilde{P}_{nit})
(\#eq:simll)
\end{equation}

El **estimador de máxima verosimilitud simulada (MSLE)** maximiza esta función:

$$\hat{\theta}_{MSL} = \arg\max_\theta SLL(\theta)$$

Este método de aproximación posee propiedades asintóticas. En particular, si $R$ crece más rápido que $\sqrt{N}$, el MSLE es consistente, asintóticamente normal y asintóticamente eficiente. En la práctica, con un valor $R = 200$ suele ser computacionalmente liviano y suficiente. La elección de la distribución $f(\beta|\theta)$ es crucial y debe basarse tanto en consideraciones teóricas como prácticas:

**Distribuciones Comunes:**

1. **Normal:** $\beta_k \sim N(b_k, \sigma^2_k)$
   - Permite valores positivos y negativos
   - Apropiada cuando no hay restricciones de signo (ej: efectos de marca)

2. **Log-Normal:** $\beta_k \sim LN(b_k, \sigma^2_k)$, es decir $\ln(\beta_k) \sim N(b_k, \sigma^2_k)$
   - Garantiza que $\beta_k > 0$ (útil para tiempo, costo)
   - **Limitación:** No permite valores negativos

3. **Normal Truncada:** Normal restringida a cierto rango
   - Útil cuando la teoría sugiere un signo pero se quiere flexibilidad
   - Ejemplo: coeficiente de precio debe ser negativo

4. **Uniforme:** $\beta_k \sim U[a_k, b_k]$
   - Útil cuando se conocen límites naturales
   - Menos común en aplicaciones de marketing

Es posible especificar correlaciones entre coeficientes aleatorios usando distribuciones multivariadas. Por ejemplo, con distribución normal multivariada:

$$\beta_n \sim N(b, \Sigma_\beta)$$

#### Descomposición de Parámetros

Es común especificar el modelo de manera que algunos parámetros sean fijos (iguales para todos los individuos) y otros sean aleatorios. Por ejemplo:

\begin{equation}
\beta_n = b + \eta_n
(\#eq:betadecomp)
\end{equation}

donde $b$ representa las preferencias medias de la población y $\eta_n \sim N(0, \Sigma_\beta)$ captura las desviaciones individuales respecto a estas preferencias medias.

Sustituyendo en la utilidad:

\begin{aligned}
u_{nit} &= (b + \eta_n)' x_{nit} + \varepsilon_{nit}\\
&= b' x_{nit} + \eta'_n x_{nit} + \varepsilon_{nit}\\
&= b' x_{nit} + \tilde{\varepsilon}_{nit}
\end{aligned}

donde $\tilde{\varepsilon}_{nit} = \eta'_n x_{nit} + \varepsilon_{nit}$ es la componente de error compuesta.

### Propiedades del Mixed Logit

El modelo mixed logit posee propiedades notables que lo hacen extremadamente flexible:

#### Aproximación Universal

**Resultado fundamental (McFadden y Train, 2000):** Cualquier modelo de utilidad aleatoria (RUM) puede ser aproximado arbitrariamente bien por un modelo mixed logit, siempre que se especifique apropiadamente la distribución de los coeficientes aleatorios.

Esto significa que el mixed logit puede aproximar patrones de sustitución arbitrarios, incluyendo aquellos del modelo probit multinomial, eliminando las restricciones del IIA cuando sea necesario.

#### Patrones de Sustitución Flexibles

A diferencia del logit estándar, el mixed logit permite patrones de sustitución no proporcionales. La elasticidad cruzada entre alternativas depende de:

- Sus características específicas
- La correlación entre sus utilidades inducida por los coeficientes aleatorios
- La distribución de preferencias en la población

**Ejemplo:** Si dos alternativas comparten muchos atributos que tienen coeficientes aleatorios correlacionados, serán sustitutos más cercanos que alternativas con atributos diferentes.

#### Captura de heterogeneidad no observable

El mixed logit permite modelar explícitamente que individuos con características observables idénticas pueden tener preferencias diferentes. La distribución $f(\beta|\theta)$ caracteriza cómo se distribuyen las preferencias en la población.

**Interpretación:**

- La **media** $b$ representa la preferencia promedio de la población
- La **desviación estándar** $\sigma_\beta$ captura el grado de heterogeneidad en preferencias
- Las **covarianzas** capturan relaciones sistemáticas entre preferencias (ej: quienes valoran más la velocidad también valoran más el confort)

#### Correlación Temporal y Entre Alternativas

Al observar decisiones repetidas del mismo individuo, el mixed logit permite correlación entre las utilidades a través del tiempo porque $\beta_n$ es constante para el individuo $n$ a lo largo de sus decisiones.

Si se observa al individuo $n$ en $T$ ocasiones y se define $y_{nit} = 1$ si elige alternativa $i$ en ocasión $t$ y 0 en caso contrario, la probabilidad condicional de la secuencia completa de elecciones es:

\begin{equation}
L_n(\beta_n) = \prod_{t=1}^T \prod_{i=1}^J L_{nit}(\beta_n)^{y_{nit}} = \prod_{t=1}^T \left[\frac{\exp(\beta'_n x_{nit})}{\sum_j \exp(\beta'_n x_{njt})}\right]^{y_{nit}}
(\#eq:seqprob)
\end{equation}

Y la probabilidad no condicional es:

\begin{equation}
P_n = \int L_n(\beta) f(\beta|\theta) d\beta
(\#eq:panelprob)
\end{equation}

Esta estructura captura naturalmente que las elecciones del mismo individuo están correlacionadas debido a sus preferencias subyacentes $\beta_n$.

### Disposición a Pagar (Willingness to Pay)

Un beneficio importante del mixed logit es que permite derivar la distribución de la **disposición a pagar (DAP)** por diferentes atributos.

Si la utilidad es lineal: $u_{nit} = \beta_{n,precio} \cdot precio_{it} + \beta_{n,atributo} \cdot atributo_{it} + ...$

La DAP por una unidad adicional del atributo es:

\begin{equation}
DAP_n = -\frac{\beta_{n,atributo}}{\beta_{n,precio}}
(\#eq:wtp)
\end{equation}

Como ambos $\beta_{n,atributo}$ y $\beta_{n,precio}$ son aleatorios, la DAP también tiene una distribución en la población. El mixed logit permite:

1. Estimar la **distribución de la DAP** en la población
2. Calcular estadísticos como la DAP media, mediana, percentiles
3. Identificar segmentos con alta/baja DAP

Si ambos coeficientes son normales, la DAP sigue una distribución de Cauchy (que puede tener momentos no definidos). Esto ha llevado a usar especificaciones alternativas como log-normal para el coeficiente de precio.

### Inferencia Individual: Conditional/Posterior Means

Una aplicación valiosa del mixed logit es la posibilidad de hacer **inferencia individual** sobre los parámetros $\beta_n$ de cada persona, incluso cuando estos no son directamente observables.

Usando el **Teorema de Bayes**, se puede calcular la distribución **posterior** de $\beta_n$ dado el historial de elecciones del individuo $n$:

\begin{equation}
h(\beta_n | y_n, \theta) = \frac{L_n(\beta_n) f(\beta_n | \theta)}{\int L_n(\beta) f(\beta | \theta) d\beta}
(\#eq:posterior)
\end{equation}

donde $y_n$ representa todas las elecciones observadas del individuo $n$.

La **media condicional** (o posterior) es:

\begin{equation}
\bar{\beta}_n = E[\beta_n | y_n, \theta] = \int \beta \cdot h(\beta | y_n, \theta) d\beta
(\#eq:condmean)
\end{equation}

Esta integral también se aproxima por simulación:

\begin{equation}
\tilde{\bar{\beta}}_n = \frac{\sum_{r=1}^R \beta^{(r)} L_n(\beta^{(r)})}{\sum_{r=1}^R L_n(\beta^{(r)})}
(\#eq:simcondmean)
\end{equation}

Tiene aplicaciones en:

- **Personalización:** Usar $\bar{\beta}_n$ para hacer recomendaciones personalizadas
- **Segmentación:** Agrupar individuos con $\bar{\beta}_n$ similares
- **Targeting:** Identificar individuos con alta probabilidad de responder a cierta oferta

### Ejemplo

Sea el caso de la elección de una marca de yogur (3 marcas: A, B, C) con datos de panel en el tiempo. Las variables que se disponen son:

- $precio_{nit}$: precio de marca $i$ para individuo $n$ en ocasión $t$
- $display_{nit}$: indicadora de display especial
- $marca_i$: indicadoras de marca (efectos fijos de marca)

La especificación del modelo corresponda a:

\begin{aligned}
u_{nit} &= \alpha_i + \beta_{n,precio} \cdot precio_{nit} + \beta_{display} \cdot display_{nit} + \varepsilon_{nit}\\
\beta_{n,precio} &\sim N(b_{precio}, \sigma^2_{precio})
\end{aligned}

En un caso hipotético, donde se estima que $b_{precio} = -2.5$ y $\sigma_{precio} = 1.0$:

- El consumidor promedio tiene sensibilidad al precio de -2.5
- Hay heterogeneidad sustancial: aprox. 68% de consumidores (equivalente a una desviación estándar) tienen sensibilidad entre -3.5 y -1.5
- Aproximadamente 1% de consumidores, calculado con la distribución normal acumulada para $b_{precio} = -2.5$, podrían tener sensibilidad positiva, ya que prefieren productos caros.

El DAP por un display se calcula como:

$$DAP_{display} = -\frac{\beta_{display}}{\beta_{n,precio}}$$

Si $\beta_{display} = 0.5$, la DAP media sería aproximadamente $0.5/2.5 = 0.20$ dólares.