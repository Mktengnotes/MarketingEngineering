# Modelos Estructurales

## Introducción a Modelos Estructurales

### Introducción

En esencia, un modelo econométrico estructural es aquel que deriva relaciones estimables estadísticamente a partir de supuestos bien definidos de comportamiento de los agentes que deciden respecto a las cantidades observables. En contraposición a los modelos estructurales están los modelos de forma reducida donde los modelos simplemente describen la variabilidad de alguna medida de interés en base a un conjunto de variables observables exógenas.

La disciplina económica suele llamar modelo estructurales a los resultantes de asumir que los consumidores maximizan una utilidad subyacente y que las firmas maximizan su rentabilidad
esperada. Desde el marketing, consideramos también en la definición en aquellos que postulan hipótesis alternativas de comportamiento incluyendo así una variedad de teorías de comportamiento que nutren la disciplina tales como teoría de prospectos, contabilidad mental, elección sobre conjuntos de consideración, etc. Como discutiremos más adelante, no existe un modelo estructural puro y la línea que los separa de los modelos de forma reducida es ciertamente difusa.

Incluiremos en nuestra discusión de modelos estructurales a cualquiera que considere alguna historia de comportamiento que permita añadir interpretabilidad a los parámetros del modelo.

**Ejemplo 1:** Supogamos que un analista busca estudiar como el precio en la región $i (p_i)$ se ve afectado por la presencia o no de competencia. Si además de los precios observamos la cantidad de clientes en la region $(POP_i)$, el ingreso per capita en la región $(INC_i)$ y un una indicatriz $CMP_i$ que toma el valor 1 si en la región correspondiente presenta competencia (0 en caso contrario).

Entonces, un modelo de forma reducida sencillo para estudiar el problema viene dado por:

$$p_i = \beta_0+ \beta_1POP_i + \beta_2INC_i + \beta_3CMP_i + \varepsilon_i$$
Bajo este enfoque, podemos usar técnicas de regresión tradicionales para estimar $β_3$ que en principio indicaría el impacto de la competencia en el nivel de precios. Sin embargo, la presencia de competencia en un determinado mercado depende también del nivel de precios. Si los precios en una región son altos, la rentabilidad esperada por entrar también es alta motivando a potenciales competidores a participar. En consecuencia, un modelo como el planteado podría subestimar
el efecto de la competencia.

Un modelo estructural buscaría derivar relaciones estimables a partir de supuestos básicos del comportamiento de la firma. Por ejemplo, podriamos asumir que cada firma decide conjuntamente la entrada/salida de un mercado y los precios a cobrar de modo de maximizar la rentabilidad esperada.

**Ejemplo 2:** Supongamos buscamos describir la productividad de los miembros de la fuerza de venta medida como número de unidades vendidad $q$.

$$q=f(X,\beta)+\varepsilon$$

La especificación del termino de error $\varepsilon$ puede por si solo permitirnos dar una interpretación estructural a los estimadores. Si simplemente asumimos un error normalmente distribuido, entonces corresponderá simplemente a un ruido blanco y la regresión simplemente nos indicará a través de los parámetros β como las variables X en promedio afectan las ventas q. Por el contrario, si asumimos que el termino $\varepsilon$ considera además del ruido una componente no observable positiva asociada a la brecha de productividad de los miembros menos eficientes de la fuerza de venta,
entonces la regresión describirá la frontera eficiente de ventas. Esto puede hacerse por ejemplo especificando que $\varepsilon = \epsilon - \xi$ donde $\epsilon$ está normalmente distribuida centrada en cero, pero $\xi$ proviene
de una normal truncada en los números positivos (este enfoque se le suele llamar de regresión estocástica de frontera).

El gran desafío de la aplicación de modelos econométricos a problemas comerciales es enriquecer el conocimiento respecto a cómo se comportan los agentes relevantes del negocio, para
así tomar decisiones más consistentes y más rentables. Desde este punto de vista, apuntamos a modelos que describan la lógica que determina el comportamiento de los clientes y firmas más allá de simples correlaciones estadísticas entre las variables observables. En general, son varias las
ventajas de usar modelos estructurales por sobre modelos de forma reducida:

1. *La capacidad de contar una mejor historia del comportamiento de los agentes*. Esto se expresa por la capacidad de interpretación directa a los parámetros del modelo. Mientras los parámetros asociados a enfoques de regresión tradicionales típicamente nos indican la magnitud en que en promedio varia alguna magnitud de interés ante variaciones de otra, los parámetros de un modelo estructural nos indican entre otros la valoración relativa de un atributo en la función de utilidad, los precios de referencia de un producto o la aversión al riesgo de un tomador de decisión. La provisión de una historia de comportamiento más completa no se deriva exclusivamente de la interpretación directa de los parámetros del modelo si no que también de la capacidad de derivar métricas complementarias tales como elasticidades y excedentes de consumidores. Más aún, podemos proyectar el comportamiento para calcular probabilidades y frecuencias de compra, participaciones de mercado, etc.

2. La generación de estimaciones consistentes con las expectativas de los analistas. Frecuentemente, al analizar los datos queremos dejar la mayor libertad posible al modelo *para dejar que la data hable*. Este enfoque puede tener valor y ser recomendable en estudios exploratorios, pero para tomar decisiones necesitamos estimaciones robustas y usar tanta información como sea posible. Las teorías usadas para derivar modelos econométricos estructurales suelen estar soportadas tanto por estudios experimentales como por amplia evidencia empírica en múltiples dominios. Por lo tanto, al incorporar teoria estamos implícitamente usando información que ha demostrado consistentemente su validez.

**Ejemplo 3:** Supongamos que queremos proponer un modelo que describa la participación de mercado de las distintas marcas en una industria. Si usamos un enfoque de regresión en que simplemente disponemos los shares al lado izquierdo y una forma funcional flexible al lado derecho, el modelo resultante podría predecir participaciones fuera del rango [0,1],que difícilmente pueden justificarse. Por el contrario si adscribimos al axioma de elección de Luce (1959) que indica que la probabilidad de elección en un determinado conjunto depende del ratio entre una medida de atracción de la alternativa con respecto al atractivo total del conjunto, forzamos a que las participaciones siempre estén en el rango deseado.

**Ejemplo 4:** La teoría económica predice que en general, las cantidades demandadas decrecen ante aumentos en su precio. Sin embargo, en muchas situaciones prácticas la disponibilidad de datos al nivel de agregación requerido es limitado dificultando la estimación de esta relación inversa entre precio y demanda. En estas situaciones no es raro que un modelo flexible prediga que la demanda crece en función del precio. Agregar estructura nos permite limitar la búsqueda solo entre aquellos modelos que son consistentes con la premisa que las demandas decrecen en el precio. 

3. *Evaluación de impacto de modificación de políticas*. Una de las herramientas fundamentales de la función comercial es la generación de planes comerciales que buscan proponer un
diseño del conjunto producto, plaza, precio y promoción que genere el mayor valor para el cliente y la captura del mayor excedente por parte de la firma. El rol de los modelos econométricos es estudiar el impacto que tendrían distintas estrategias en el comportamiento del consumidor. En esencia, plan de marketing propone un cambio en las reglas del juego
que han generado la data que observamos y por tanto necesitamos apuntar a estimar los elementos más básicos del comportamiento que se mantendrán invariantes ante modificación de productos, precios, canales de distribución, etc. En este grupo tenemos, valoraciones por atributos de productos, costo de transporte, aversión al riesgo, entre otros, que no pueden ser estimados a menos que derivemos el modelo a partir de teorías individuales de comportamiento. En otras palabras, la derivación de modelos de demanda a partir de teorías comportamiento nos permiten evaluar contrafactuales que apoyan el diseño de propuestas
de valor efectivas.

La necesidad de evaluar contrafactuales usando elementos fundamentales que no se vean afectados por cambios en los sistemas fue inicialmente discutido por Robert Lucas (1976) en
la famosa crítica que lleva su nombre. En el contexto de la predicción de efectos macroeconómicos, Lucas postuló que cualquier cambio en las políticas variaran sistemáticamente la
estructura de los modelos y por tanto debemos apuntar a describir parámetros profundos que gobiernan comportamiento individual.

**Ejemplo 5:** Consideren un retailer que vende múltiples productos a través de dos canales, las salas de venta tradicionales y un sitio web con despacho directo. El retailer esta evaluando la posibilidad de re-asignar el conjunto de productos que vende a través de cada canal para aumentar la rentabilidad del negocio. Para apoyar esta decisión, parece evidente que el simple analisis de las ventas de cada producto en cada canal no nos ayudara a predecir como dichos productos se venderían en el otro canal o cómo se afectaría la venta si un producto deja de venderse en algunos de los canales. Para hacer este ejercicio necesariamente necesitaremos investigar primitivas más fundamentales del comportamiento como preferencias intrínsecas por canal para cada categoría y patrones de sustitución entre las alternativas disponibles
dentro del canal y con respecto al otro canal.

**Ejemplo 6:** En muchas industrias como la de vestuario de moda o de artículos tecnológicos, hay una alta variabilidad de la oferta con constante entradas y salidas de diferentes versiones de los productos dificultando la proyección del desempeño de cada variante en el tiempo. Mientras el surtido de producto varía con frecuencia, hay parámetros de la demanda pueden perdurar por varias temporadas tales como la elasticidad al precio, crecimiento de la categoría, factores estacionales y de substitución/complementariedad de atributos. Un enfoque estructural apunta precisamente a la estimación de estos parámetros estables.

4. *Testear aplicabilidad de teoría.* Al usar un enfoque estructural, nos forzamos a pensar detalladamente respecto al problema y explicitar cada una de los supuestos de comportamiento. Las especificaciones alternativas de modelos de forma reducida simplemente corresponden a formas funcionales diferentes y por tanto no son informativas respecto a lógica en que deciden los agentes. Por otra parte, dos modelos estructurales diferentes provienen de supuestos de comportamiento diferentes y por tanto cuando uno de ellos ajusta mejor a la data nos indica que hay una teoría de comportamiento es más plausible que la otra en el dominio
de aplicación del modelo. Así, los modelos estructurales no solo se nutren de teoría sino que también ayudan a su desarrollo.

Las ventajas antes descritas no implican que siempre debieran preferirse modelos estructurales por sobre los de forma reducida. Como hemos descrito, los modelos de forma reducida suelen proveer suficiente flexibilidad para dejar que sea la data la que hable, lo que puede ser particularmente util en análisis exploratorios del caso bajo estudio. Además, muchas veces la inclusión de más estructura en el modelo implica rutinas de estimación más sofisticada siendo con frecuencia
altamente intensivas computacionalmente.

Es importante destacar que no existe un modelo puramente estructural. Todo modelo requiere en algún momento suponer alguna forma funcional flexible sin fundamento teórico sólido. Por ejemplo, podemos asumir que los consumidores al elegir un producto están maximizando una utilidad subyacente, pero ¿Cómo describimos dicha función de utilidad? ¿Qué variables explicativas usamos y cual forma funcional escogemos?  Ciertamente la especificidad de las teorías disponibles no alcanza a responder a estas preguntas y debemos por lo tanto escoger en base a la intuición y empíricamente entre aquellas que generen mejor ajuste y/o capacidad de pronóstico. De esta forma, un buen modelo debe balancear adecuadamente el uso de la teoría con la simpleza y flexibilidad del modelo.

Para ser convincente, un modelo estructural debe al menos (i) entregar suficiente flexibilidad para aprender de la data, (ii) derivar las ecuaciones de comportamiento de razonables respectos de los agentes involucrados y (iii) incorporar explícitamente en la descripción la naturaleza no experimental de la data.

**Observación:** En nuestra discusión, hemos hecho la distinción entre modelos probabilísticos y modelos estructurales. Aunque los modelos probabilísticos proveen una historia de comportamiento de los agentes, los supuestos básicos usados para derivarlos no se sustentan en ninguna
teoría de comportamiento. Por ejemplo, en modelos de duración en tiempo discreto solemos suponer que los clientes dejan de estar activos con cierta probabilidad. Más que una teoría de comportamiento esto es simplemente una descripción probabilística de un fenómeno. En determinadas situaciones, especialmente en casos en que no disponemos una descripción rica del ambiente en que se los agentes toman sus decisiones, nos conformamos con esta descripción agregada del comportamiento. El enfoque estructural sobre el que ahondaremos en esta parte resulta particularmente útil cuando tenemos suficiente información para investigar las motivaciones profundas de las elecciones. Al definir un modelo estructural, tanto las teorías de comportamiento como la descripción probabilística del sistema son fuentes válidas de estructura. Sin embargo, consideraremos como modelo econométrico estructural a aquellos que se nutren de ambas fuentes.

### Modelos Estructurales en Marketing

El desarrollo de modelos estructurales se ha gestado en varias areas del conocimiento tales como Economía, Transportes, Logística, Finanzas y Marketing. Entre estas áreas, la del marketing se ha constituido en un terreno particularmente fértil para el desarrollo y adopción del enfoque estructural. Identificamos al menos cuatro motivos por los cuales la adición de estructura en los modelos econométricos son particularmente útiles para el análisis de problemas comerciales:

1. *Disponibilidad de Data*. Gran parte de la data que registran las compañías dan cuenta de las interacciones entre clientes y firma como son ocasiones de compra, visitas a sitios web corporativos o llamadas a los call center. De esta forma, un conjunto importante de la data disponible dentro de las organizaciones son informativos respecto a procesos claves de la función comercial. Así, los requerimientos de datos impuestos por los modelos estructurales están inmediatamente satisfechos por procesos operacionales.

2. *Atractivo de la Evaluación de la Intervención de sistemas*. En la función comercial, casi por definición buscamos perturbar los sistemas para mejorar la oferta de valor cambiando precios, proponiendo nuevos diseños de productos, redefiniendo la cadena logística, etc.). De esta forma necesitamos disponer de modelos que describan la reacción de los consumidores ante dichos cambios del ambiente competitivo lo que, de acuerdo a la crítica de Lucas, solo puede hacerse con un modelo estructural.

3. *Importancia de Heterogeneidad*. En Marketing buscamos hacer inferencia desagregada a nivel de cliente o segmento para poder diseñar versiones especializadas del marketing mix que sea atractivo para segmentos específicos de clientes. Como los modelos estructurales requieren especificar los supuestos de comportamiento a nivel individual, la generación de
estimaciones desagregadas suele derivarse directamente.

4. *Pragmatismo en la aceptación de teorías*. Como hemos argumentado, una de las ventajas de los modelos estructurales es que nos permite testear si una determinada teoría de comportamiento aplica a una situación. A diferencia de otras disciplinas, en marketing hay una tradición de una revisión continua de las fuerzas que moldean el comportamiento de las personas y por tanto el enfoque de modelos estructurales entrega una herramienta alternativa a la verificación experimental de nuevas teorias.

### Taxonomía de Modelos Estrucuturales

Metodológicamente, es útil generar un clasificación de los tipos de modelos estructurales existentes en la literatura. Como hemos consignado, uno de los costos de la inclusión de teoría en modelos econométricos es la mayor complejidad en las rutinas de estimación. Es esta complejidad la que dificulta la generación un mecanismo único que permita estimar modelos generales y por tanto nos vemos forzados a usar metodologías específicas dependiendo de la naturaleza del problema. En nuestra discusión basaremos nuestra clasificación en la evaluación de cuatro factores.

1. *Nivel de agregación de la Data*. Hemos propuesto que un modelo estructural debe basarse en una descripción detallada de los supuestos de los tomadores de decisión a nivel individual. Por lo tanto la disponibilidad de data a nivel individual como la decisión de compra de cada uno de los individuos de un panel de consumidores, nos habilita para, imponiendo las restricciones de identificación necesaria, estimar los parámetros de comportamiento de manera mas o menos directa. Sin embargo, en ciertas situaciones solo se dispone de información agregada, como participaciones de mercado o datos agregados de venta. En estos casos, la identificación de parámetros de comportamiento requiere además de una descripción del mecanismo mediante el cual se agregan las decisiones individuales. Este mecanismo típicamente considera la especificación de un modelo de heterogeneidad describiendo como se distribuyen los parámetros entre los clientes la que se integra sobre la población para generar las métricas agregadas. Esto es precisamente lo propuesto por el método BLP (a partir de
Berry, Levinsohn y Pakes quienes primero propusieron el método en 1995) que describe un método que basado en un modelo logit permite estimar ofertas y demandas de un modelo oligopólico con información agregada. Por simplicidad, en esta versión nos concentraremos en modelos estimables directamente sobre data desagregada a nivel individual.

2. *Temporalidad de las Decisiones*. Dependiendo de la amplitud temporal considerada por los agentes al evaluar las alternativas de decisión distinguiremos entre problemas estáticos y dinámicos. Básicamente, si consideramos que las acciones que observamos resultan de una evaluación completa del horizonte, entonces hablaremos de problemas dinámicos. En caso contrarios diremos que el problema es estático. La distinción es importante desde un punto de vista metodológico. Si el tomador de decisiones basa sus decisiones exclusivamente
mirando el pasado, entonces estas decisiones pueden caracterizarse directamente mediante condiciones de optimidad sencillas. Por el contrario, si el tomador de decisión además evalúa las repercusiones (inciertas) que sus acciones de hoy podrían tener en su bienestar futuro, entonces necesitamos caracterizar las políticas optimas a través de ecuaciones de Bellman que incorporen explícitamente la naturaleza multiperiodo del problema. En este caso, para encontrar la política óptima del problema se requiere usar técnicas como programación dinámica estocástica o control óptimo aumentando de manera importante la complejidad computacional de la estimación.

3. *Naturaleza de las Variables de Decisión*. Si las variables sobre las que deciden los agentes son continuas (gasto, montos de inversión, unidades compradas, etc.), hablaremos de un
modelo de decisión continuo. Si las variables sobre las que deciden los agentes son discretos (si visita o no visita la tienda, si elige la marca A o marca B, etc.), hablaremos de un modelo de decisión discreto. La distinción es relevante en cuanto las soluciones de un problema de decisión continua puede caracterizarse directamente mediante condiciones de KarushKuhn-Tucker mientras que las soluciones de un problema de decisión discreta requieren una enumeración del valor de las alternativas.

4. *Identidad de los Agentes*. Los modelos estructurales pueden usarse para estudiar tanto el comportamiento de los clientes o de las otras firmas en el mercado. El área que estudia el comportamiento de las firmas ha tenido una gran desarrollo en los últimos años y se conoce como Organización Industrial Empírica. En esta versión, concentraremos la discusión en el estudio de los clientes por dos motivos principales: la disponibilidad de data de comportamiento de cliente y la simpleza de las nociones de equilibrio requeridas para describir a los clientes. Mientras cada cliente suele tener poco poder de mercado por si mismo, las acciones de marketing de las firmas competidoras típicamente pueden modificar de manera importante las condiciones del mercado. Así, la descripción de las decisiones de las firmas conlleva desafíos metodológicos importantes como la inclusión de nociones sofisticadas de equilibrio para internalizar que las decisiones de las firmas resultan tanto de mirar las respuestas esperadas de los clientes como las reacciones estratégicas de los competidores.

Metodológicamente es útil también distinguir los métodos de estimación de los modelos. La literatura reconoce dos grandes enfoques para estimar modelos estructurales como los aquí presentados: Método de los Momentos Generalizados (GMM) y Método de la Máxima Verosimilitud. Dada su eficiencia estadística (en el sentido que usa toda la información disponibles), en esta primera versión usaremos solo el método de la máxima verosimilitud. En lo que sigue nos enfocaremos la discusión al estudio del comportamiento de clientes, en problemas estáticos (o con dinámica limitada a la incorporación del pasado) y con data desagregada. Partiremos describiendo brevemente modelos de decisión continuos para luego iniciar una discusión más extensa en modelos de decisión discreta que tienen una tradición más larga en marketing.



## Logit

### Modelos de Elección Discreta

Un modelo de elección discreta consiste básicamente en situaciones en que la naturaleza de las variables de decisión a las que se enfrenta el tomador de decisión son discretas. Para ilustrar la intuición de la diferencia con respecto a modelos de decisión continua es útil pensar que mientras
estos últimos buscan describir decisiones de el cuanto, los modelos de elección discreta se concentran en el cuál. La distinción además relevante desde un punto de vista metodológico. A diferencia de los modelos de elección continua en que la optimidad de la elección queda bien descrita por
condiciones de primer orden, al enfrentar decisiones discretas caracterizaremos la optimidad por enumeración. Ejemplos típicos en que la decisión a evaluar es de naturaleza discreta incluye la elección de una marca por sobre otra en la góndola de un supermercado, la decisión de visitar o no a una tienda, la elección del color de una prenda de vestir, de un canal de venta y la elección de las firmas respecto a entrar o no entrar a un mercado.

Para que un problema de elección discreta este bien definido necesitamos además de variables de decisión discretas, que el conjunto de alternativas presente las siguientes tres características:

1. *EXHAUSTIVAS*:  El conjunto sobre el que los tomadores de decisión eligen deben incluir todas las alternativas posibles. En otras palabras, cualquiera sea la decisión observada, debe
estar incluida en el conjunto de elección. Esta condición es poco restrictiva ya que siempre es posible incluir en el set de alternativas la posibilidad “ninguna de las anteriores” o similar que por definición incluya toda las otras posibilidades no consideradas en conjunto. Sin embargo, esta estrategia debe usarse con precaución. Por ejemplo, al estudiar la elección de marca en una categoría en que observamos que los clientes no siempre compran alguna
de las marcas disponibles podríamos incluir la alternativa de no compra en el conjunto de elección. Si la proporción de no compras es alta en nuestra muestra, la inclusión de la alternativa de no compra podría limitar la habilidad del modelo de aprender respecto a como los clientes eligen entre marcas. En este caso, podría convenir concentrarse en la elección de la marca condicional en haber hecho una compra en la categoría.

2. *MUTUAMENTE EXCLUYENTES:* El conjunto de decisión debe definirse de modo que en cada ocasión el tomador de decisión seleccione solo una de las alternativas disponibles. Esto es,
la elección de una alternativa implica necesariamente la no elección de cualquiera de las alternativas restante. Aunque aparentemente restrictiva, la definición de conjunto de elección puede acomodarse para generar conjuntos mutuamente excluyente. Por ejemplo, consideremos un modelo para describir la elección de los clientes entre la *tienda física tradicional* o la *tienda virtual*. Si simplemente una alternativa de elección por cada canal, entonces excluimos
la posibilidad que un mismo cliente más de un canal en un mismo periodo. Para incorporar esta posibilidad debiéramos redefinir las alternativas agregando la opción de *tienda tradicional y virtual*.

3. *FINITAS*: El conjunto debe contener un conjunto finito de alternativas. Esta condición es importante por dos motivos técnicos. Primero, un conjunto finito facilita la evaluación de la optimidad de las decisiones y segundo, facilita la definición de probabilidades de elección. Existen situaciones que la decisión teóricamente permite infinitas posibilidades, pero que en la practica se concentran en un numero reducido de alternativas y por tanto quedan bien representadas por un modelo de elección discreta. Por ejemplo, podemos usar el número de cajas de cereal compradas por los clientes en cada visita al supermercado. Aunque teóricamente los clientes siempre podrían comprar una unidad adicional, el problema queda bien descrito considerando solo las alternativas de 0,1,2,3 o más de 3 cajas. 

El comportamiento observado de los agentes es que alternativa eligieron en cada oportunidad y por tanto los modelos de elección discreta se enfocan en describir la probabilidad de elección de cada alternativa. Aunque frecuentemente nos encontraremos con situaciones en que solo observamos una decisión por agente, a continuación describiremos el caso de panel en que observamos múltiples agentes tomando decisiones en múltiples períodos.

Un modelo estructural para describir la probabilidad de elegir cada alternativa necesita especificar el mecanismo que usan los agentes para decidir entre las alternativas. Partiremos asumiendo que en cada oportunidad de compra $t$, el tomador de decisión $n$ elige la alternativa $i$ que le reporta mayor utilidad $u_{nit}$. Aunque el tomador de decisión necesariamente necesita conocer la utilidad que deriva de cada una de las alternativas, desde la perspectiva del analista solo observamos algunas características del ambiente de decisión y del tomador de decisión a partir de las cuales podemos intentar aproximar la utilidad del tomador de decisión a través de una función $v_{nit}(x_{nit}, θ)$ donde $x_{nit}$ son las características observables del problema y θ el vector de parámetros que buscamos estimar y que describen la relación de dichas características con la utilidad.

**Ejemplo:** Supongamos que queremos describir la elección del medio de pago que usan los usuarios de una tienda determinado, el que permite pagar en efectivo o con alguna tarjeta bancaria. El analista observa 3 variables que intuye pueden ser relevantes en la elección del medio de pago: el género del cliente ($F_n = 1$ si cliente es de género femenino), su nivel de ingresos $(I_n)$ y el monto de la transacción $(M_{nt})$. Son precisamente estas características las que estarían incluidas en la matriz que hemos llamado $x_{nit}$. A partir de esta información pueden plantearse múltiples modelos para describir $v_{nit}$ (asumiremos que $i = 0$ corresponde al caso de pago con efectivo mientras que $i = 1$ al de pago con tarjeta).

* *Modelo Lineal Homogéneo*: Aquí, la utilidad para ambas alternativas crece linealmente con las variables observables. En este caso, los parámetros son los mismos para todos los tomadores de decisión y por tanto el vector de parámetros viene dado por $θ = (α_0, α_1, β, γ, δ)$

$$v_{nit} = \alpha_i + \beta F_n + \gamma I_n + \delta M_{nt}$$

* *Modelo Lineal Heterogéneo:* Aquí, la utilidad para ambas alternativas también crecen linealmente con las variables observables, pero ahora los parámetros varían por alternativa y por agente y por tanto el vector de parámetros viene dado por $\theta = ( \{ \alpha_{1n} \} _{n=1}^{N}, \beta_0,\beta_1,\gamma_0,\gamma_1,\{ \delta_n \}_{n=1}^{N} )$ ^[Como veremos, para identificar el problema necesitamos imponer que $α_{0n} = 0 ∀n = 1, ..., N$]

 $$v_{nit} = \alpha_{in} + \beta_i F_n + \gamma_i I_n + \delta_n M_{nt}$$
 
La definición que los interceptos dependen del cliente n simplemente nos indica que cada cliente tiene una preferencia intrínseca por cada medio de pago. Del mismo modo, estamos
imponiendo que la influencia que tiene el monto en el atractivo que tiene cada alternativa depende del cliente. Por ejemplo, mientras para algunos clientes el monto de la transacción puede jugar un rol importante en la decisión del medio de pago, para otros este efecto podría no ser relevante. Por último, la dependencia de la alternativa en los parámetros asociados a género e ingreso podrían usarse para por ejemplo situaciones en que el nivel de ingreso afecta el atractivo de un medio de pago pero no del otro (la intuición para el género es análoga).

Por supuesto, también podemos postular modelos no lineales u otras especificaciones de la heterogeneidad. Por ejemplo que la influencia del ingreso varíe por medio de pago, pero que
el efecto del género sea constante entre las alternativas. Descubrir la especificación que mejor describe el problema es precisamente la tarea del analista.

**Observación:** En el ejemplo hemos introducido brevemente el concepto de heterogeneidad. Sin embargo, para facilitar la exposición de los temas básicos, en primera instancia nos concentraremos en modelos sin heterogeneidad. En marketing los modelos que incluyen heterogeneidad en las preferencias son tan importantes que postergaremos su discusión en una capitulo separado.

En la práctica, aún en situaciones en que observamos con detalle el ambiente de decisión, no podremos describir con exactitud todas las factores que gobiernan el comportamiento de los agentes. Por lo tanto, definiremos $\varepsilon _{nit}$ como el error (aditivo) que cometemos al aproximar $u_{nit}$ a
través de $v_{nit}$.

$$u_{nit} = v_{nit} + \varepsilon_{nit}$$
Así, descomponemos la utilidad de cada alternativa en una componente sistemática (u observable o explicable) $v_{nit}$ y en una componente aleatoria (o no observable o inexplicable) $\varepsilon_{nit}$. Como veremos, la tarea de modelamiento del problema involucra tanto la especificación de la componente sistemática como de la aleatoria.

La componente básica para estimar estadísticamente un modelo de elección discreta es la especificación de la probabilidad de elección de cada alternativa. Sea $P_{nit}$ la probabilidad que el agente $n$ escoja la alternativa $i$ en la oportunidad de compra $t$. El supuesto de maximización de utilidades
implica que $P_{nit}$ puede escribirse como:

\begin{aligned}
P_{nit} &= Pr(u_{nit}>u_{njt},\forall j \neq i)\\
&= Pr(v_{nit} + \varepsilon_{nit} >v_{njt} + \varepsilon_{njt} ,\forall j \neq i)\\
&= \int \textbf{1} (\varepsilon_{njt} - \varepsilon_{nit} > v_{nit} - v_{njt}) f(\varepsilon_{nt}) d \varepsilon_{nt}
\end{aligned}

donde $\textbf{1}(\cdot)$ toma el valor 1 si se cumple el argumento y el valor 0 en caso contrario. En esta expresión, $\varepsilon_{nt} = (\varepsilon_{n1t},\varepsilon_{n2t}, ...,\varepsilon_{nIt})$ es el vector de las componentes aleatorias de la elección del agente $n$ en la oportunidad $t$ y $f(·)$ la función de densidad que describe su comportamiento
probabilístico. La elección de la distribución de la componente aleatoria es importante en cuanto impone restricciones a los patrones de comportamientos que pueden ser capturados por el modelo. Concentraremos nuestra atención en los casos en que $\varepsilon_{nit}$ se distribuye valor extremo que da origen al modelo *logit* y normal que da origen al modelo *probit*.

### Modelo Logit

El modelo logit resulta de asumir que cada $\varepsilon_{nit}$ es independientemente distribuido de acuerdo a una distribución gumbel o de valor extremo tipo I.

\begin{equation} 
  \begin{array}{cc}
  F(\varepsilon_{nit}) = e^{-e^{-\varepsilon_{nit}}}
   &f(\varepsilon_{nit}) = e^{-\varepsilon_{nit}}e^{-e^{-\varepsilon_{nit}}}
   \end{array}
  (\#eq:logituno)
\end{equation} 

Aplicando esta definición, podemos demostrar que la probabilidad de elección en un modelo logit corresponde a una fórmula cerrada sencilla (para el detalle de la derivación ver [apéndice](#apéndice)):

\begin{aligned}
P_{nit} &= \int Pr(\varepsilon_{njt} < v_{nit} - v_{njt} + \varepsilon_{nit}, \forall j \neq i | \varepsilon_{nit}) f(\varepsilon_{nit})d\varepsilon_{nit}\\
&= \int \left(\prod_{j \neq i} e^{-e^{-(v_{nit} - v_{njt} + \varepsilon_{nit})}}\right) e^{-\varepsilon_{nit}}e^{-e^{-\varepsilon_{nit}}}d \varepsilon_{nit} \\
&= \frac{e^{v_{nit}}}{\sum_j e^{v_{njt}}}
\end{aligned}

En algunos libros de texto se justifica esta expresión simplemente como una regresión logística, esto es una transformación lineal para normalizar la utilidad de modo de interpretarla directamente como una probabilidad de elección en el rango [0,1]. Aunque válido, resulta útil entender que
en efecto dicha expresión puede derivarse a partir de supuestos de maximización de utilidades.

Para ganar algo de intuición respecto a la expresión de la probabilidad de elección, es útil graficarla con respecto a la utilidad derivada por cada alternativa. Por ejemplo, supongamos que tenemos una decisión binaria que por ejemplo corresponde a decisión de comprar o no comprar un producto. En este caso, la probabilidad de comprar el producto crece *sigmoidalmente* con la utilidad derivada de la compra. Esto es, al graficar la probabilidad de compra con respecto a la
utilidad derivada obtenemos una curva S como muestra la Figura 1. En la figura, hemos agregado también la curva de la probabilidad de elección en el caso en que en vez de asumir que el error se distribuye valor extremo como demanda el modelo logit, asumimos que el error está normalmente
distribuido como tradicionalmente hacemos en otros modelos econométricos.

```{r probelec, fig.cap="Probabilidad de elección",out.width='50%', fig.align='center'}
knitr::include_graphics(rep("images/probabilidad_eleccion.png"))
```

\FloatBarrier

La disposición de una fórmula cerrada para la probabilidad de elección facilita el cálculo de múltiples métricas asociadas que permiten complementar el análisis. Supongamos que la utilidad de una alternativa viene dada por $v_{nit} = v(x_{nit}, θ)$, entonces podemos calcular:

+ Como varía la probabilidad de elegir la alternativa $i$ al variar alguna componente de la utilidad de la misma alternativa.

$$\frac{dP_{nit}}{dx_{nit}}  = \frac{\partial v_{nit}}{\partial x_{nit}} \cdot P_{nit}(1- P_{nit})$$

+ Como varía la probabilidad de elegir la alternativa $i$ al variar alguna componente de la utilidad otra alternativa.

$$\frac{dP_{nit}}{dx_{njt}}  = \frac{\partial v_{njt}}{\partial x_{njt}} \cdot P_{nit}\cdot P_{njt}$$

+ Elasticidad de la probabilidad de elegir la alternativa $i$ con respecto alguna componente de la utilidad de la misma alternativa.

$$e_{ix_{nit}} = \frac{\partial P_{nit}}{\partial x_{nit}}  \cdot \frac{x_{nit}}{P_{nit}}= \frac{\partial v_{nit}}{\partial x_{nit}} x_{nit} (1- P_{nit})$$

+ Elasticidad de la probabilidad de elegir la alternativa $i$ con respecto alguna componente de la utilidad otra alternativa.

$$e_{ix_{njt}} = \frac{\partial P_{nit}}{\partial x_{njt}}  \cdot \frac{x_{njt}}{P_{nit}}= \frac{\partial v_{njt}}{\partial x_{njt}} x_{njt} P_{njt}$$

Recuerden que unas de las motivaciones para el uso de modelos estructurales es la posibilidad de analizar contrafactuales, esto es ver que pasaría con el mercado si hay cambio en alguna
variable de control interesante. Por ejemplo que pasa con las participaciones de mercado si sube el precio de una alternativa, si se aumenta la frecuencia publicitaria, etc. Las métricas recién presentadas permiten precisamente hacer dichas evaluaciones de manera directa.

#### Propiedades del modelo Logit

El modelo logit es bastante flexible para acomodar una amplia variedad de situaciones. En efecto, distintas especificaciones de las funciones de utilidades de las alternativas permiten describir múltiples fenómenos asociados a la elección. Sin embargo, es importante reconocer que los supuestos subyacentes al logit imponen importantes restricciones a como describimos la lógica en que los agentes evalúan las alternativas y escogen entre ellas.

Para fijar ideas, resulta útil pensar qué restricciones impone asumir que las componentes no observables de la utilidad son todas independientes entre ellas. El supuesto de independencia
nos obliga a imponer que cualquier relación entre las utilidades de dos alternativas debe necesariamente capturarse a través de variables observables. Del mismo modo, las utilidades que derivamos por dos alternativas en ocasiones de elección diferentes solo pueden describirse a través de elementos que podamos observar a lo largo del tiempo. Para entender mejor como estas limitaciones se materializan en la formulación del modelo, discutiremos formalmente tres características del modelo logit: la existencia de patrones de substitución proporcional, la incapacidad de capturar tanto heterogeneidad aleatoria en las preferencias como componentes dinámicas no observables.

##### Patrones de sustitución {-}

Los patrones de substitución derivados de un modelo logit son bastante peculiares y aunque desde un punto de vista econométrico puede resultar beneficioso, desde el punto de vista de la investigación de teorías de comportamiento suele ser considerado como bastante restrictivo. Entenderemos por patrones de substitución a la forma en que cambia la probabilidad de elección de alguna alternativa cuando se modifica el atractivo de otra alternativa. Para entender la naturaleza de los patrones de substitución del modelo logit es util calcular el ratio de las probabilidades de elección de dos alternativas cualquiera $i$ y $j$.

$$\frac{P_{ni}}{P_{nj}} = e^{v_{ni} - v_{nj}}$$

Este ratio solo depende de las utilidades observables de las dos alternativas consideradas lo que indica que la probabilidad relativa de elegir la alternativa $i$ sobre la alternativa $j$ no depende de que otras alternativas existan ni de los atributos que ellas tengan. Por ejemplo, si agregamos una alternativa al conjunto de elección, el ratio de probabilidades de las alternativas existentes se mantendrá constante independiente de las características de la nueva alternativa. Nos referiremos a esta característica como *independencia de alternativas irrelevantes* o *IIA*.

Para ejemplificar consideremos una botillería que ofrece dos variedades de vino, uno blanco y otro tinto. Supongamos ademas que estas dos alternativas tienen la misma participación de
mercado, esto es la mitad de los clientes de la botillería compra vino blanco y la otra mitad compra vino tinto. En este caso, las utilidades sistemáticas debieran ser similares y por tanto el ratio de probabilidades de elección de vino blanco sobre vino tinto debiera acercarse a 1. Motivado por un
mayor margen de los vinos tintos, el administrador de la botillería decide incorporar una nueva variedad de vino tinto. Intuitivamente esperaríamos que, como la nueva variedad de vino tinto es un sustituto más cercano al tinto existente, la participación de mercado de este debiera decrecer
más que la de vino blanco. Sin embargo la propiedad de IIA impone que este ratio se mantiene constante. En otras palabras, la introducción de una nueva alternativa disminuirá la participación de todas la otras alternativas independiente de las similitudes que tengan. Esta última observación
puede corroborarse calculando la elasticidad de sustitución Eiznj que determina como cambia la probabilidad de consumir la alternativa $i$ ante un cambio en un atributo $z_{nj}$ de la alternativa $j$. 

$$E_{ix_{nj}} = -\frac{\partial v_{nj}}{\partial x_{nj}}x_{nj} P_{nj}, \forall i \neq j$$
 
Notamos en esta expresión que la expresión no depende de $i$ por lo que es constante para todas las alternativas de elección. Luego, si ocurre una mejora en los atributos de una alternativa la probabilidad de elección de las demás disminuye en el mismo porcentaje independiente de la similitud entre alternativas. Nos referiremos a esta característica como *patrones de sustitución proporcionales*.

Una ventaja de los patrones de substitución del modelo logit es que permite que los parámetros del modelo sean estimados consistentemente en base a un subconjunto de las alternativas. Esto es particularmente útil en ambientes de decisión de marketing donde típicamente nos encontramos con centenas de productos que potencialmente pueden constituir alternativas de elección en una situación de compra. De esta forma, para estimar un modelo logit podemos seleccionar conjuntos
reducidos de alternativas que capturan los elementos esenciales de la elección e ignorar que pasa con todas las otras alternativas.

##### Incapacidad de estimar componentes aleatorias {-}

La investigación de las diferencias entre las preferencias de los distintos clientes es un tema fundamental para el desarrollo de planes comerciales exitosos. Tradicionalmente distinguimos dos tipos de heterogeneidad de acuerdo a la capacidad de observación del analista. Por un lado tenemos el estudio de heterogeneidad observable que indica como las preferencias de los tomadores de decisiones varían de acuerdo a sus características medibles. Este tipo de heterogeneidad
nos permite por ejemplo estudiar diferencias en las preferencias entre hombres y mujeres, por edad o por niveles de ingreso. Sin embargo una proporción importante de las diferencias de las preferencias no es atribuible a características observables como las recién descritas. Por ejemplo, dos hermanos del mismo género de edades similares viviendo en el mismo hogar pueden tener preferencias completamente diferentes respecto a sabores de yogur.

El resultado fundamental en esta sección indica que un modelo logit permite estudiar variaciones de preferencias asociadas a componentes observables, pero no a componentes no observables.
Para ilustrar este resultado, supongamos un tomadores de decisión caracterizados por la siguiente función de utilidad:

$$u_{nit} = \alpha_i + \beta_np_{it} + \varepsilon_{nit}$$
 
 Es decir, la utilidad de cada alternativa tiene una componente base que es constante entre los tomadores de decisión y una penalización por precio $p_{it}$ al que se enfrenta el tomador de decisión. Al indexar $β_n$ por agente estamos explícitamente permitiendo que algunos tomadores decisión sean más sensibles al precio que otros. Supongamos que postulamos que el coeficiente precio viene dado por la siguiente ecuación de regresión.
 
 $$\beta_n = \lambda_0 + \lambda_1I_n + \mu_n$$
 Donde $λ_0$ captura la sensibilidad base al precio, $I_n$ el nivel de ingreso del agente $n$ y $λ_1$ el coeficiente que indica como dichos niveles de ingresos afectan la sensibilidad al precio. Por último $\mu_n$ es un valor aleatorio que captura todas las otras componentes que modifican la sensibilidad al precio más allá del nivel base y los ingresos.
 
\begin{aligned}
u_{nit} &= \alpha_i + (\lambda_0 + \lambda_1 I_n + \mu_n)p_{it} + \varepsilon_{nit}\\
&= \alpha_i + \lambda_0 p_{it} + \lambda_1p_{it}I_n + \xi_{nit}
\end{aligned}
 
 
 Donde $\xi_{nit} = \mu_np_{it} + \varepsilon_{nit}$. De esta expresión debiera ser claro que la inclusión de heterogeneidad
observable puede ser capturada bajo un enfoque logit. En efecto, los parámetros $α_i$, $λ_0$ y $λ_1$ dan cuenta respectivamente del nivel de utilidad base por alternativa, de la penalización por precio y de como dicha penalización se ve modificada por el nivel de ingresos. Lamentablemente, la
variación aleatoria µn no puede ser incluida ya que su inclusión necesariamente implica que las componentes errores $\xi_{nit}$ no están idénticamente distribuidas. En efecto, se puede mostrar que $\mathbb{V}ar(\xi_{nit}, \xi_{njt}) = \mathbb{V}ar(\mu_n)p^2_{it}$ que evidentemente varía entre alternativas. Más aún, también se puede mostrar que $\mathbb{C}ov(\xi_{nit}, \xi_{njt}) = \mathbb{V}ar(\mu_n)p_{it}p_{jt} \neq 0 $violando también el supuesto de independencia.

 Es importante notar que la incapacidad de capturar aleatoriedad aplica también a componentes dinámicas. Esto es, al observar compras repetidas en el tiempo, el modelo logit no permite capturar que hay componentes no observables que varíen en el tiempo. Por ejemplo no podemos incorporar que, debido a factores externos no observables, en algunos periodos algunas alternativas son más atractivas para todos los agentes decidiendo en dichos periodos. Al igual que en el ejemplo anterior, incluir estas variaciones viola los supuestos de distribuciones independientes e idénticamente distribuidas para las componentes no observables.
 
### Estimación

Para estimar el modelo, necesitamos escribir la verosimilitud del problema. La componente fundamental para la construcción de la verosimilitud es la descripción de la probabilidad de
elección $P_{nit}$. Para el caso del modelo logit, como la expresión de la probabilidad de elección corresponde a una fórmula analítica cerrada, la construcción de la verosimilitud es directa. Si la componente determinística de la utilidad viene dada por $v_{nit}(x_{nit}, θ)$ y si $y_{nit}$ es una variable que toma valor 1 si el tomador de decisión $n$ escoge alternativa $i$ en oportunidad $t$, entonces la verosimilitud viene dada por:

$$L(\theta) = \prod_n\prod_i\prod_t (P_{nit})^{y_{nit}} =  \prod_n\prod_i\prod_t \left(\frac{e^{v_{nit}(x_{nit},\theta)}}{\sum_j e^{v_{njt}(x_{njt},\theta)}}\right)^{y_{nit}} $$
La que podemos maximizar directamente usando rutinas estándares de programación convexa. Computacionalmente, suele ser más conveniente trabajar con la log-verosimilitud en vez de la verosimilitud. Esto porque la multiplicación de probabilidades genera muy rápidamente valores que computacionalmente son indistinguibles de cero. Recordar que el valor de los valores óptimos son invariantes a transformaciones monótonas como la del logaritmo. 

\begin{aligned}
LL(\theta) &= \sum_n\sum_i\sum_t y_{nit} ln \left(\frac{e^{v_{nit}(x_{nit},\theta)}}{\sum_j e^{v_{njt}(x_{njt},\theta)}}\right)\\
&= \sum_n\sum_i\sum_t y_{nit}v_{nit}(x_{nit},\theta) - \sum_n\sum_i\sum_t ln \left(\sum_j e^{v_{njt}(x_{njt},\theta)}\right)
\end{aligned}

Como hemos indicado, esta función objetivo puede ser ingresada directamente a cualquier rutina de optimización para encontrar los estimadores máximo verosímiles. Para muchas instancias prácticas, es conveniente contar además con las derivadas de la log-verosimilitud de modo de encontrar eficientemente direcciones de máximo ascenso o evaluar si el punto es estacionario o no. Afortunadamente, para la mayoría de las especificaciones del modelo logit, estas derivadas también son fáciles de obtener. Por ejemplo, si la componente sistemática de la utilidad viene dada por $v_{nit}(x_{nit}, θ) = x'_{nit}θ$ entonces


$$\frac{\partial LL(\theta)}{\partial \theta} = \sum_n\sum_i\sum_t \left(y_{nit} - \frac{e^{x'_{nit}θ}}{\sum_j e^{x'_{njt}θ}}\right)x_{nit}$$

Del mismo modo, podemos calcular segundas derivadas que resultan útiles para el cálculo de errores estándares de los parámetros.

#### Evaluación del modelo {-}

Al igual que en otros modelos econométricos, una de las componentes fundamentales del análisis es la evaluación de la calidad del modelo. La variedad de métricas disponibles para la evaluación es muy amplia y la mayoría son transversales a cualquier modelo. Categorizaremos las herramientas de evaluación en tres grupos: bondad de ajuste, capacidad de pronóstico y test de hipótesis.

1. *BONDAD DE AJUSTE*: Las métricas de bondad de ajuste básicamente nos indican que tan bien el modelo ajusta a la data. En el contexto de modelos de regresión, solemos analizar el estadístico $R^2$ que mide la proporción de la variabilidad de la variable dependiente que puede ser explicado por la variación de las variables independientes. En el contexto de modelos de elección discreta basaremos la evaluación en el valor de la verosimilitud usando alguno o varios de los siguientes indicadores:

  - $\rho$ de McFadden. Este índice está en el rango [0,1] e informalmente, se suele interpretar como el coeficiente de determinación $(R^2)$ en el sentido que un valor cercano a 0 indica un mal ajuste y un valor cercano a 1 indica un buen ajuste. Sin embargo, es importante notar que no puede decirse que $\rho$ mida la variabilidad explicada por el modelo como
hace el coeficiente de determinación

    $$\rho=1 - \frac{LL(\hat{\beta})}{LL(0)}$$

  - Criterio de información de Akaike(AIC) y Bayesiano (BIC): Uno de las limitaciones del ρ de McFadden es que solo permite comparar modelos con el mismo numero de parámetros. Los dos indicadores más usados para comparar modelos con distintos números de parámetros son AIC y BIC en que se penaliza la verosimilitud por el número de parámetros para capturar el hecho que al incluir nuevos parámetros la verosimilitud necesariamente crecerá. La diferencia entre AIC y BIC es que el primero tiene una penalización constante por numero de parámetros mientras que la penalización del segundo depende de la cantidad de data disponible. Si la log verosimilitud de un modelo con n observaciones y k parámetros es LL entonces AIC y BIC vienen dados por: 
  
\begin{array}{cc}
AIC = -LL(\hat{\theta} + 2k) & BIC =-2LL(\hat{\theta} + kln(n))
\end{array}

2. *CAPACIDAD DE PRONÓSTICO*: Un modelo que explique muy bien la data puede correr el riesgo de sobre ajustar. Esto es que no permita describir el fenómeno más allá de los datos
con que se calibran. Para medir la capacidad de pronóstico se suele dividir la data en un subconjunto de calibración en que estimamos el modelo y otro de validación en que comparamos las realizaciones con lo pronosticado usando las estimaciones del subconjunto de calibración. Supongamos que estamos interesados en evaluar la capacidad de pronostico de un indicador $f_{ni}$ que puede corresponder a las elecciones mismas, participaciones de mercado o cualquier otra. Si $\hat{f}_{ni}$ es el pronostico del modelo entonces solemos usar el mean
absolute error (MAE) o el mean absolute percentage error (MAPE) 

\begin{array}{cc}
MAE = \frac{1}{N} \sum_n \sum_i |f_{ni} - \hat{f}_{ni}| &MAPE = \frac{1}{N} \sum_n \sum_i \left| \frac{f_{ni} - \hat{f}_{ni}}{f_{ni}} \right|
\end{array}

3. *TEST DE HIPÓTESIS*: La evaluación de hipótesis, también puede contribuir a diagnosticar un modelo. Por ejemplo, al agregar una variable explicativa, nos gustaría evaluar si el coeficiente correspondiente es significativamente diferente de 0, lo que podemos hacer directamente a través de la construcción de intervalos de confianza o su estadístico $t$ equivalente (recordar que la varianza de del estimador máximo verosímil puede obtenerse usando el inverso del Hessiano). En ocasiones también estaremos interesados en testear hipótesis más complejas para lo que recurrimos a test de ratios de verosimilitud. Supongamos por ejemplo que tenemos un modelo en que los coeficientes asociado a display difieren por marca para incorporar la posibilidad que algunas de ellas sean más efectivas en su comunicación en sala. El test de ratios de verosimilitud nos permite por ejemplo testear si estos coeficientes son iguales o si efectivamente difieren entre marcas. Si la hipótesis nula puede expresarse como $k$ restricciones sobre los parámetros, entonces podemos estimar un modelo A no restringido y otro B restringido y calculamos el estadístico $LR = 2(LLA − LLB)$, que se distribuye $\chi^2$
con k grados de libertad.

### Apéndice{-}

**Derivación probabilidad de elección modelo logit**

Por definición 

$$P_{nit} = Pr(\varepsilon_{njt}<v_{nit} - v_{njt} + \varepsilon_{nit}), \forall j \neq i$$
Fijando el valor de $\varepsilon_{nit}$, la probabilidad anterior no es más que una multiplicación de funciones distribución de variables aleatorias valor extremo. Por lo tanto podemos condicionar en $\varepsilon_{nit}$ y luego integrar respecto a los valores que puede tomar. Para simplificar la notación, sea $s=\varepsilon_{nit}$


\begin{aligned}
P_{nit} &= \int_{-\infty}^{\infty} \left(\prod_{j\neq i}    e^{-e^{-(s + v_{ni} - v_{nj})}}\right) e^{-s} e^{-e^{-s}}ds\\
&= \int_{-\infty}^{\infty} \left(\prod_{j}   e^{-e^{-(s + v_{ni} - v_{nj})}}\right) e^{-s}ds\\
&= \int_{-\infty}^{\infty} exp\left(\sum_{j}  e^{-(v_{ni} - v_{nj})}\right) e^{-s}ds
\end{aligned}

Para resolver la integral podemos recurrir a un cambio de variables $t = e^{−s}$ y $dt = e^{−s}ds$. Con esto

\begin{aligned}
P_{nit} &= \int_{\infty}^{0} -e^{t\sum_{j}  e^{-(v_{ni} - v_{nj})}} dt\\
&= \frac{ e^{-(v_{ni} - v_{nj})}}{\sum_{j}  e^{-(v_{ni} - v_{nj})}}\mid^{\infty}_{0}\\
&= \frac{e^{v_{ni}}}{\sum_j e^{v_{nj}}}
\end{aligned}


## Probit

### Definición

Al introducir modelos de elección discreta, postulamos que los tomadores de decisiones disponían de una función de utilidad subyacente que descomponíamos en una componente determinística y otra aleatoria. Más aún, discutimos que el modelo que describe la probabilidad de elegir cada una de las alternativas quedaba directamente determinada por la distribución que asumiéramos para la componente aleatoria de la utilidad. Aunque una especificación de errores normales
centrados en cero tiene una larga tradición en modelos econométricos, por simplicidad optamos iniciar la discusión con modelos *logit* derivados de asumir que la componente aleatoria de la utilidad se distribuía valor extremo tipo I. En este capitulo volveremos al caso de componentes
aleatorias normales que dan origen al modelo probit. Formalmente, un modelo *probit* resulta de los siguientes supuestos de comportamiento:

\begin{equation} 
  \begin{array}{cc}
  u_{ni} = v_{ni} + \varepsilon_{ni} &\varepsilon_n \sim N(0,Σ)
  \end{array}  
  (\#eq:compo)
\end{equation} 

La normalidad de los errores provee bastante flexibilidad para acomodar una amplia variedad de estructuras de las preferencias. Como veremos en la discusión que sigue, un modelo con errores normales permite acomodar factores sistemáticos no observables en la utilidad. Una de las pocas limitaciones de un modelo probit viene de la normalidad dichos factores. Por ejemplo, si queremos incorporar el efecto que tiene el precio en la utilidad como una componente aleatoria, entonces las colas de la distribución normal implicara una probabilidad positiva de que algunos clientes aumenten la utilidad de una alternativa si aumenta el precio de esta. Formalmente, el supuesto de la normalidad de la componente aleatoria de la utilidad implica que su función de densidad viene dada por:

\begin{equation} 
  \phi(\varepsilon_n) = \frac{1}{(2\pi)^{I/2}|Σ|^{1/2}}e^{-\frac{1}{2} \varepsilon'_n Σ^{-1}\varepsilon_n} 
  (\#eq:density)
\end{equation} 

Esta expresión no es más que la versión multivariada de la bien conocida densidad de la distribución $N(0, \sigma^2)$. La matriz Σ corresponde a la matriz varianza-covarianza de los errores. Por tratarse de una distribución normal, la matriz Σ es simétrica y de dimensión I × I, donde I es el número de alternativas disponibles para el tomador de decisión. Por ejemplo, si hay tres alternativas disponibles, la matriz Σ tomaría la siguiente forma:

\begin{equation} 
  Σ = \begin{bmatrix} \sigma_{11} & \sigma_{12} & \sigma_{13}\\
 \cdot & \sigma_{22} & \sigma_{23}\\
 \cdot & \cdot & \sigma_{33}
\end{bmatrix} 
  (\#eq:matrix)
\end{equation} 

Los coeficientes en la diagonal dan cuenta de la variabilidad de la componente aleatoria de la utilidad. Así por ejemplo, si $σ_{ii}$ tiene un valor alto indica que hay una fracción importante de la utilidad de la alternativa $i$ que no es capturada por el modelo de la componente sistemática. Los
coeficientes fuera de la diagonal dan cuenta de la correlación de las componentes no observables de cada una de las alternativas. De este modo, si $σ_{ij}$ tiene un valor positivo alto indica que existe un elemento no observable importante que afecta simultáneamente las alternativas $i$ y $j$.

Como vimos en el desarrollo del modelo *logit*, una componente fundamental para estimar un modelo de elección discreta es la derivación de una expresión para la probabilidad que cada agente elija cada alternativa en cada ocasión. Para el modelo probit, la probabilidad que el individuo $n$ elija la alternativa $i$ viene dada por:

\begin{equation} 
  \begin{aligned}
    P_{ni} &= Pr (v_{ni} + \varepsilon_{ni} > v_{nj} + \varepsilon_{nj}), \forall j \neq i \\
    &= \int \textbf{1} (v_{ni} + \varepsilon_{ni} > v_{nj} + \varepsilon_{nj})\phi(\varepsilon_n)d\varepsilon_n , \forall j \neq i
  \end{aligned}  
  (\#eq:modprobit)
\end{equation} 

Intuitivamente, simplemente calculamos el volumen bajo la densidad $\phi(\varepsilon_n)$ en la región en que los errores son tales que la alternativa $i$ es aquella que reporta mayor utilidad al individuo $n$. A diferencia del modelo logit, la integral sobre la densidad $\phi(\cdot)$ no tiene primitiva analítica y por tanto no disponemos de una formula cerrada para $P_{ni}$.

### Patrones de substitución

Una de las grandes ventajas de un modelo *probit* es su flexibilidad para capturar una amplia variedad de patrones de comportamiento. En efecto, un modelo *probit* no impone restricciones en los patrones de substitución más allá de la simetría propia de la distribución normal lo que posibilita al analista explorar el esquema que mejor se ajusta a la data. En este sentido, es útil compararlo con el modelo *logit* que, aunque provee una fórmula analítica cerrada para la probabilidad de cada elección, impone la propiedad de substitución proporcional (o de independencia de alternativas irrelevantes). El modelo probit no tiene esta propiedad y por tanto el aumento de la probabilidad de elección de una alternativa puede tener impactos diferentes en las probabilidades de elección de las alternativas remanentes. Esto permitiría por ejemplo identificar pares de alternativas que son mejores substitutos (complementos) más allá de las comonalidades que podrían existir en las componentes determinísticas de su utilidad. 

A continuación discutiremos como el modelo probit puede ser usado para representar algunas situaciones de elección discreta.

#### Variación aleatorias en preferencias

Una de las componentes más importantes en el diseño de un plan comercial exitoso es la identificación de como las preferencias de los potenciales clientes se distribuyen en la población. Identificando estas variaciones, podemos encontrar las propuestas de valor que resulten más atractivas para cada grupo de clientes. En un modelo probit, podemos asumir que los parámetros que definen la componente determinística son heterogéneos en la población sin perder los supuestos básicos que definen el modelo. Por simplicidad, supongamos que la componente determinística de la utilidad es lineal:

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = \beta'_{n} x_{ni} + \varepsilon_{ni} & \varepsilon_n \sim N(0,Σ)
  \end{array}  
  (\#eq:cdeterminista)
\end{equation} 

Notar que a diferencia de los modelos anteriores, ahora hemos asumido que cada tomador de decisión $n$ tiene su propio set de parámetros $β_n$ que describen sus preferencias por las alternativas disponibles. Para completar el modelo necesitamos especificar una distribución de $β_n$ en la población. Para mantener la estructura del modelo asumiremos normalidad: $β_n ∼ N(b, σ^2_β)$. Dado que la suma de dos variables aleatorias normales se distribuye normal, es fácil ver que el modelo es
equivalente a

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = b'_{n} x_{ni} + \eta_{ni} & \eta_n \sim N(0,\hat{Σ})
  \end{array}  
  (\#eq:cdeterministaequi)
\end{equation} 

Las componentes de la matriz de varianza-covarianza resultante $\hat{Σ}$ pueden trazarse directamente a las componentes de la matriz Σ original como lo indica el siguiente ejemplo:

**Ejemplo:** Consideremos un modelo de elección con dos alternativas y un modelo lineal con una única variable para describir la componente sistemática de la utilidad. En este caso, las utilidades por cada alternativa vienen dadas por:

\begin{aligned}
u_{n1} &= \beta_n x_{n1} + \varepsilon_{n1}\\
u_{n2} &= \beta_n x_{n2} + \varepsilon_{n2}
\end{aligned}

donde $\varepsilon_{n1}$ y $\varepsilon_{n2}$ son términos independientes e idénticamente distribuidos con varianza $σ_\varepsilon$. Si asumimos que el parámetro $β_n$ se distribuye normal con media $b$ y varianza $σ_β$, entonces podemos re-escribir las utilidades como:

\begin{aligned}
u_{n1} &= b x_{n1} + \eta_{n1}\\
u_{n2} &= b x_{n2} + \eta_{n2}
\end{aligned}

donde $\eta_{n1}$ y $\eta_{n2}$ están normalmente distribuidas. Cada una tiene esperanza cero: $\mathbb{E}(\eta_{ni}) =\mathbb{E}(β_nx_{ni} + ε_{ni}) = 0$, varianza igual a $\mathbb{V}ar(\eta_{ni}) = \mathbb{V}ar(β_n x_{ni} + ε_{ni}) = x^2_{ni}σ_β + σ_ε$ y covarianzas $\mathbb{C}ov(\eta_{n1}, \eta_{n2}) = x_{n1}x_{n2}σ_β$. Así, la matriz de covarianza viene dada por:

\begin{aligned}
Σ &= \begin{bmatrix}x^2_{n1}σ_β + σ_ε & x_{n1}x_{n2}σ_β\\
x_{n1}x_{n2}σ_β & x^2_{n2}σ_β + σ_ε \end{bmatrix} \\
&= \sigma_\beta \begin{bmatrix}x^2_{n1} & x_{n1}x_{n2}\\
x_{n1}x_{n2} & x^2_{n2} \end{bmatrix} + \sigma_{\varepsilon} \begin{bmatrix}1 & 0\\
0 & 1\end{bmatrix}
\end{aligned}

El siguiente paso es estimar. Recordando que el comportamiento no es afectado por transformaciones multiplicativas de la utilidad, es necesario escalar esta matriz. Lo recomendable es fijar $σ_ε = 1$, obteniendo así

$$Σ= \sigma_\beta \begin{bmatrix}x^2_{n1} & x_{n1}x_{n2}\\
x_{n1}x_{n2} & x^2_{n2} \end{bmatrix} + \begin{bmatrix}1 & 0\\
0 & 1\end{bmatrix}$$


#### Dependiencia del tiempo

Hemos discutido que bajo un modelo *probit* podemos estudiar relaciones no observables entre las alternativas de elección. En las bases disponibles para la función comercial, las observaciones suelen estar indexadas temporalmente generando estructuras de panel que permiten estudiar aspectos interesantes de los agentes. Discutiremos a continuación cómo usar un modelo *probit* para explorar no solo relación entre las utilidades de alternativas sino que también del comportamiento de las utilidades de las alternativas en el tiempo. Al igual que en la sección anterior, buscamos
encontrar patrones temporales en las componentes no observables de la utilidad, ya que las variaciones en la componente observable puede ser fácilmente estudiada incluyendo variables observables que describan la evolución temporal del sistema. Por ejemplo, si creemos que la utilidad
de una de las alternativas es creciente en el tiempo, basta incluir el tiempo $t$ entre las variables independientes en la descripción de la utilidad de la alternativa. En general, debiéramos esperar que las utilidades estén correlacionadas tanto en el tiempo como entre las alternativas ya que los
factores que no son observados por el analista suelen ser persistentes en el tiempo. Eventualmente un modelo *probit* también podría ayudar a identificar shocks en que hay variaciones instantáneas (o de unos pocos periodos) en las utilidades de varias de las alternativas. 

Supongamos que observamos un panel de $N$ clientes que deciden respecto de $I$ alternativas en $T$ periodos y que la utilidad del producto que el agente $n$ deriva sobre la alternativa $i$ en el periodo $t$ viene dada por:

\begin{equation} 
  \begin{array}{cc}
    u_{ni} = v_{nit} + \varepsilon_{nit}  & [\varepsilon_{n11},...,\varepsilon_{nI1},\varepsilon_{n12}, ..., \varepsilon_{nI2},...,\varepsilon_{n1T},...,\varepsilon_{nIT}] \sim N(0,Σ)
  \end{array}  
  (\#eq:cdeterministaequi)
\end{equation} 

La matriz de covarianza Σ tiene dimensión $IT ×IT$ (como veremos, no todas las componentes son identificables y tendremos que imponer ciertas restricciones). Para paneles típicos, $T$ es grande y generan matrices de varianza covarianza muy grandes. Por ejemplo, si tenemos datos semanales de compras de 5 marcas por un periodo de 2 años, nos
enfrentaremos con una matriz de varianza (sin normalizar) con 5 × 104 = 520 filas y 520 columnas, lo que nos generaría no solo un modelo difícil de estimar numéricamente si no que también difícil de interpretar. Así, para usar un modelo *probit* con dependencia en el tiempo, típicamente
agregaremos estructura al modelo. Por ejemplo podemos restringir nuestro análisis a grupos de periodos que podría ser el caso de las decisiones antes y después de una intervención en el sistema (e.g. antes y después del lanzamiento de una campaña publicitaria).

**Ejemplo:** Supongamos un caso de elección binaria, el error está compuesto por una componente sistemática específica del tomador de decisión, y otra que es variable en el tiempo.


\begin{equation} 
  \varepsilon_{nt} = \eta_n + \mu_{nt}
  (\#eq:errorbinario)
\end{equation} 

Si asumimos que $η_n$ está distribuida $N(0, σ)$ y $\mu_{nt}$ en $N(0, 1)$, entonces la varianza y covarianza son

  \begin{equation} 
  \mathbb{V}ar(\varepsilon_{nt}) =  \mathbb{V}ar(\eta_{n}+ mu_{nt}= \sigma + 1)
    (\#eq:varbinaria)
  \end{equation} 
  
  \begin{equation} 
  \mathbb{C}ov(\varepsilon_{nt},\varepsilon_{ns})) =  \mathbb{E}((\eta_{n}+ mu_{nt}) (\eta_{n}+ mu_{ns})) = \sigma
    (\#eq:covbinaria)
  \end{equation} 

La matriz Σ, por lo tanto, es

\begin{equation} 
  Σ = \begin{bmatrix}\sigma +1 & \sigma & ... & \sigma\\
\sigma & \sigma+1 & ... & \sigma\\
\vdots & \vdots & \ddots & \vdots\\
\sigma & \sigma & \dots & \sigma +1
\end{bmatrix}
    (\#eq:matrizsum)
  \end{equation} 

### Identificación

Para estimar un modelo *probit*, junto con los parámetros de la componente sistemática de la utilidad necesitamos estimar los coeficientes la matriz Σ. Por tratarse de una distribución normal,la matriz Σ es simétrica y por tanto en principio se deben estimar $\frac{I(I+1)}{2}$ de sus componentes. Sin
embargo dicho problema no es identificable y necesitamos imponer restricciones adicionales. La intuición detrás de esta falta de identificación resulta de asumir que las utilidades subyacentes que maximizan los individuos son monótonas y homotéticas. En otras palabras, podemos agregar un valor constante a las utilidades de cada una de las alternativas o escalarlas en cualquier proporción y la identidad de la alternativa de mayor utilidad no cambia. En general, si tenemos I alternativas, solo podemos identificar $\frac{I(I+1)}{2} -1$ parámetros. A continuación discutiremos dos enfoques para generar restricciones que hagan el problema identificable.

#### Normalización de las funciones de utilidad

Motivados en las propiedades de la función de utilidad, este enfoque consiste en imponer directamente restricciones de escala y locación. Este enfoque es completamente general y permite además garantizar identificación con un procedimiento estándar que puede incluso automatizarse. Formalmente el proceso consiste en imponer dos restricciones:

1. FIJAR LOCACIÓN: Como el valor absoluto de las utilidades es irrelevante, podemos fijar arbitrariamente el punto de referencia sobre el cual interpretaremos las utilidades. De esta forma, tomaremos la utilidad de una de las alternativas como referencia y re-definiremos las utilidades como las diferencias con respecto a la alternativa de referencia.

2. FIJAR ESCALA: Como la escala de las utilidades es irrelevante, podemos fijarla asignando un valor arbitrario a cualquiera de las componentes de la matriz de varianza covarianza. Típicamente impondremos que la primera componente de la diagonal tome el valor 1.
 
**Ejemplo:** Consideremos la normalización de una matriz Σ resultante de un problema de elección discreto de 4 alternativas. 

\begin{equation} 
  Σ = \begin{bmatrix} \sigma_{11} & \sigma_{12} & \sigma_{13} & \sigma_{13}\\
 \cdot & \sigma_{22} & \sigma_{23} & \sigma_{24}\\
 \cdot & \cdot & \sigma_{33} & \sigma_{34}\\
 \cdot & \cdot & \cdot & \sigma_{44}
\end{bmatrix} 
  (\#eq:matrixcuatro)
\end{equation} 


El primer paso en la normalización es considerar diferencias de utilidades con respecto a una alternativa de referencia, la que por simplicidad escogeremos como la primera de la lista. Al fijar esta utilidad y tomar las diferencias, hemos reducido la dimensión del vector errores, resultando en una matriz de varianza-covarianza $\hat{Σ} = \{\hat{σ}_{ij}\}^3_{i,j=1}$ cuyas componentes vienen dadas por:

\begin{aligned}
\hat{\sigma}_{22} &= \sigma_{22} + \sigma_{11} - 2\sigma_{12}\\
\hat{\sigma}_{33} &= \sigma_{33} + \sigma_{11} - 2\sigma_{13}\\
\hat{\sigma}_{44} &= \sigma_{44} + \sigma_{11} - 2\sigma_{14}\\
\hat{\sigma}_{23} &= \sigma_{23} + \sigma_{11} - \sigma_{12} - \sigma_{13}\\
\hat{\sigma}_{24} &= \sigma_{24} + \sigma_{11} - \sigma_{12} - \sigma_{14}\\
\hat{\sigma}_{34} &= \sigma_{34} + \sigma_{11} - \sigma_{13} - \sigma_{14}
\end{aligned}

El segundo paso en la normalización es fijar en 1 (o cualquier otro real positivo) una de las componentes de la diagonal de la matriz de varianza covarianza para precisar la escala de la función de utilidad. Por simplicidad escogemos la primera componente de la diagonal. Para hacerla 1 basta con dividir toda la matriz por dicha componente, resultando en una matriz de varianzacovarianza  $\hat{Σ} = \{\hat{σ}_{ij}\}^3_{i,j=1}$cuyas componentes vienen dadas por:

\begin{aligned}
\hat{\sigma}_{33} &= \frac{\sigma_{33} + \sigma_{11} - 2\sigma_{13}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{44} &= \frac{\sigma_{44} + \sigma_{11} - 2\sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{23} &= \frac{\sigma_{23} + \sigma_{11} - \sigma_{12} - \sigma_{13}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{24} &= \frac{\sigma_{24} + \sigma_{11} - \sigma_{12} - \sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}\\
\hat{\sigma}_{34} &= \frac{\sigma_{34} + \sigma_{11} - \sigma_{13} - \sigma_{14}}{\sigma_{22} + \sigma_{11} - 2\sigma_{12}}
\end{aligned}

La matriz resultante $\hat{Σ} es identificable. En ella es importante trazar sus componentes originales de la matriz sigma porque nos ayudan a darle interpretación a los resultados obtenidos en la estimación.

#### Incorporación de restricciones estructurales

Aunque completamente general, la normalización descrita en la sección anterior, muchas veces puede ser algo inconveniente en cuanto los parámetros estimados no tienen interpretación
directa. Un enfoque que permite interpretar directamente los parámetros se obtiene al imponer estructura sobre la matriz de varianza-covarianza a partir de supuestos de comportamiento. Por ejemplo, podemos imponer que las componentes aleatorias de algunos pares de alternativas no están correlacionadas o que algún grupo de alternativas tiene la misma variabilidad de la componente no observable. El cuadro 4.1 ejemplifica algunas de las estructuras de varianza-covarianza comúnmente usadas en la literatura.

Otros modelos usados en la literatura y que están implementadas en aplicaciones comerciales incluyen estructuras de bandas, Huynh-Feldt, autoregresivo heterogéneo y simetría compuesta. Como en otros aspectos de la modelación, la elección de la estructura a elegir para la matriz de
varianza-covarianza dependerá de las hipótesis de comportamiento que tengamos a la mano y la dificultad numérica de estimar el modelo resultante. 




## *Mixed Logit*
